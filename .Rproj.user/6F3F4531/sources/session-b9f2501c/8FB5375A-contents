---
title: "Predictive Policing - Technical Implementation"
subtitle: "MUSA 5080 - Fall 2025"
author: "Chloe Robinson"
date: today
format:
  html:
    code-fold: show
    code-tools: true
    toc: true
    toc-depth: 3
    toc-location: left
    theme: cosmo
    embed-resources: true
editor: visual
execute:
  warning: false
  message: false
---

## About This Exercise

Spatial predictive model for burglaries using count regression and spatial features.

# Setup

```{r setup}
#| message: false
#| warning: false

# Load required packages
library(tidyverse)      # Data manipulation
library(sf)             # Spatial operations
library(here)           # Relative file paths
library(viridis)        # Color scales
library(terra)          # Raster operations (replaces 'raster')
library(spdep)          # Spatial dependence
library(FNN)            # Fast nearest neighbors
library(MASS)           # Negative binomial regression
library(patchwork)      # Plot composition (replaces grid/gridExtra)
library(knitr)          # Tables
library(kableExtra)     # Table formatting
library(classInt)       # Classification intervals
library(here)
library(broom) 

# Spatstat split into sub-packages
library(spatstat.geom)    # Spatial geometries
library(spatstat.explore) # Spatial exploration/KDE

# Set options
options(scipen = 999)  # No scientific notation
set.seed(5080)         # Reproducibility

# Create consistent theme for visualizations
theme_crime <- function(base_size = 11) {
  theme_minimal(base_size = base_size) +
    theme(
      plot.title = element_text(face = "bold", size = base_size + 1),
      plot.subtitle = element_text(color = "gray30", size = base_size - 1),
      legend.position = "right",
      panel.grid.minor = element_blank(),
      axis.text = element_blank(),
      axis.title = element_blank()
    )
}

# Set as default
theme_set(theme_crime())
```

# Part 1: Load and Explore Data

## Exercise 1.1: Load Chicago Spatial Data

```{r load-boundaries, message=FALSE, warning=FALSE, results='hide'}

# Load police districts (used for spatial cross-validation)
policeDistricts <- 
  st_read("https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON") %>%
  st_transform('ESRI:102271') %>%
  dplyr::select(District = dist_num)

# Load police beats (smaller administrative units)
policeBeats <- 
  st_read("https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON") %>%
  st_transform('ESRI:102271') %>%
  dplyr::select(Beat = beat_num)

# Load Chicago boundary
chicagoBoundary <- 
  st_read("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson") %>%
  st_transform('ESRI:102271')

```


## Exercise 1.2: Load Burglary Data

```{r load-burglaries, message=FALSE, warning=FALSE,  results='hide'}
#| message: false
#| warning: false

# Load from provided data file (downloaded from Chicago open data portal)
burglaries <- st_read("data/burglaries.shp") %>%
  st_transform("ESRI:102271")

# Check the data
cat("\n✓ Loaded burglary data\n")
cat("  - Number of burglaries:", nrow(burglaries), "\n")
cat("  - CRS:", st_crs(burglaries)$input, "\n")
cat("  - Date range:", min(burglaries$date, na.rm = TRUE), "to", 
    max(burglaries$date, na.rm = TRUE), "\n")
```

**Question 1.1:** How many burglaries are in the dataset? What time period does this cover? Why does the coordinate reference system matter for our spatial analysis?

There were 7,482 burglaries reported in Chicago in 2017. The coordinate reference system used for this spatial analysis is ESRI:102271 (Illinois State Plane East, NAD83, US Feet). This is the appropriate coordinate reference system for Chicago, as it is designed specifically for the Illinois East region, so spatial distortion is minimized, and the unit of measurment is US survey feet, the most common unit in US planning and engineering. It also uses a projected coordinate system, allowing for accurate distance and area calculations - which the longitude/latitude CRS can't do reliably. Using this CRS ensures that the results for the spatial operations conducted on burglaries in Chicago are reliable and interpenetrate.

## Exercise 1.3: Visualize Point Data

```{r visualize-points}
#| fig-width: 10
#| fig-height: 5

#Visualise Burglaries 
# Simple point map
p1 <- ggplot() + 
  geom_sf(data = chicagoBoundary, fill = "gray95", color = "gray60") +
  geom_sf(data = burglaries, color = "#d62828", size = 0.1, alpha = 0.4) +
  labs(
    title = "Burglary Locations",
    subtitle = paste0("Chicago 2017, n = ", nrow(burglaries))
  )

# Density surface using modern syntax
p2 <- ggplot() + 
  geom_sf(data = chicagoBoundary, fill = "gray95", color = "gray60") +
  geom_density_2d_filled(
    data = data.frame(st_coordinates(burglaries)),
    aes(X, Y),
    alpha = 0.7,
    bins = 8
  ) +
  scale_fill_viridis_d(
    option = "plasma",
    direction = -1,
    guide = "none"  # Modern ggplot2 syntax (not guide = FALSE)
  ) +
  labs(
    title = "Density Surface",
    subtitle = "Kernel density estimation"
  )

# Combine plots using patchwork (modern approach)
p1 + p2 + 
  plot_annotation(
    title = "Spatial Distribution of Burglaries in Chicago",
    tag_levels = 'A'
  )
```

**Question 1.2:** What spatial patterns do you observe? Are burglaries evenly distributed across Chicago? Where are the highest concentrations? What might explain these patterns?

Several factors likely contribute to this uneven distribution. Neighborhoods with higher population and housing density naturally create more opportunities for burglary compared to areas with fewer homes. Mixed-use or commercial corridors with steady foot traffic may also attract offenders. Socioeconomic conditions also play a role, areas facing economic disadvantage often see higher crime, while wealthier neighborhoods can become targets because of the higher value of property. Historical policing patterns may also influence where incidents are reported or detected, shaping how the patterns are on the map.

# Part 2: Create Fishnet Grid

## Exercise 2.1: Understanding the Fishnet

```{r create-fishnet}
# Create 500m x 500m grid
fishnet <- st_make_grid(
  chicagoBoundary,
  cellsize = 500,  # 500 meters per cell
  square = TRUE
) %>%
  st_sf() %>%
  mutate(uniqueID = row_number())

# Keep only cells that intersect Chicago
fishnet <- fishnet[chicagoBoundary, ]
```

**Question 2.1:** Why do we use a regular grid instead of existing boundaries like neighborhoods or census tracts? What are the advantages and disadvantages of this approach?

A regular fishnet grid was generated across the Chicago boundary with 500 × 500 meter cells, producing 2,458 cells in total, each covering approximately 250,000 m² (0.25 km²). Only cells that intersect the city boundary were retained, ensuring that all grid squares fall within Chicago. A fishnet grid is used to provide a uniform spatial framework, avoiding the inconsistencies of boundaries such as neighborhoods or census tracts. Using grids makes it easier to compare different areas evenly as it uses equal-sized spatial units. However, this approach does have limitations. Grids can split local areas or include cells that cover uninhabited zones like water, parks or industrial areas. Despite the limitations, the fishnet grid is valuable for data-driven spatial modelling, as it reduces bias due to predefined administrative boundaries.

## Exercise 2.2: Aggregate Burglaries to Grid

```{r aggregate-burglaries}

# Spatial join: which cell contains each burglary?
burglaries_fishnet <- st_join(burglaries, fishnet, join = st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID) %>%
  summarize(countBurglaries = n())

# Join back to fishnet (cells with 0 burglaries will be NA)
fishnet <- fishnet %>%
  left_join(burglaries_fishnet, by = "uniqueID") %>%
  mutate(countBurglaries = replace_na(countBurglaries, 0))

# Summary statistics
summary(fishnet$countBurglaries)
cat("\nCells with zero burglaries:", 
    sum(fishnet$countBurglaries == 0), 
    "/", nrow(fishnet),
    "(", round(100 * sum(fishnet$countBurglaries == 0) / nrow(fishnet), 1), "%)\n")
```

```{r visualize-fishnet}
#| fig-width: 8
#| fig-height: 6

# Visualize aggregated counts
ggplot() +
  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +
  geom_sf(data = chicagoBoundary, fill = NA, color = "white", linewidth = 1) +
  scale_fill_viridis_c(
    name = "Burglaries",
    option = "plasma",
    trans = "sqrt",  # Square root for better visualization of skewed data
    breaks = c(0, 1, 5, 10, 20, 40)
  ) +
  labs(
    title = "Burglary Counts by Grid Cell",
    subtitle = "500m x 500m cells, Chicago 2017"
  ) +
  theme_crime()
```

**Question 2.2:** What is the distribution of burglary counts across cells? Why do so many cells have zero burglaries? Is this distribution suitable for count regression? (Hint: look up overdispersion)

After aggregating burglaries to the 500m × 500m fishnet grid, there was a clear variation in how crime was distributed across the city. The burglary counts across the grid are uneven. The majority of the cells have very few or zero burglaries. Burglary counts ranged from 0 to 40 per cell, with a mean of 3.52. About 24% of all grid cells recorded zero burglaries, showing that many parts of the city experience little or no burglary activity in 2017. The much higher maximum value shows that burglaries are not evenly spread out as there are clear distinct hotspot areas where incidents are concentrated. This pattern reflects the typical clustering seen in crime data, where a small number of places account for a large share of events. This distribution is suitable for count regression, but it is likely overdispersed, meaning that the varience is larger than the mean. Overdispersion is common in spatial crime data because counts cluster in certain locations rather than being evenly distributed.

# Part 3: Create a Kernel Density Baseline

Before building complex models, let's create a simple baseline using **Kernel Density Estimation (KDE)**.

**The KDE baseline asks:** "What if crime just happens where it happened before?" 

```{r kde-baseline}
#| message: false

# Convert burglaries to ppp (point pattern) format for spatstat
burglaries_ppp <- as.ppp(
  st_coordinates(burglaries),
  W = as.owin(st_bbox(chicagoBoundary))
)

# Calculate KDE with 1km bandwidth
kde_burglaries <- density.ppp(
  burglaries_ppp,
  sigma = 1000,  # 1km bandwidth
  edge = TRUE    # Edge correction
)

# Convert to terra raster (modern approach, not raster::raster)
kde_raster <- rast(kde_burglaries)

# Extract KDE values to fishnet cells
fishnet <- fishnet %>%
  mutate(
    kde_value = terra::extract(
      kde_raster,
      vect(fishnet),
      fun = mean,
      na.rm = TRUE
    )[, 2]  # Extract just the values column
  )

```

```{r visualize-kde}
#| fig-width: 8
#| fig-height: 6

#Plot
ggplot() +
  geom_sf(data = fishnet, aes(fill = kde_value), color = NA) +
  geom_sf(data = chicagoBoundary, fill = NA, color = "white", linewidth = 1) +
  scale_fill_viridis_c(
    name = "KDE Value",
    option = "plasma"
  ) +
  labs(
    title = "Kernel Density Estimation Baseline",
    subtitle = "Simple spatial smoothing of burglary locations"
  ) +
  theme_crime()
```

**Question 3.1:** How does the KDE map compare to the count map? What does KDE capture well? What does it miss?

The KDE represents our null hypothesis that burglaries happen where they happened before, with no other information. The KDE map gives a smoother visual of burglary patterns than the raw count map, clearly highlighting the major hot-spot areas and shows broader clusters across the city. However, KDE also loses some important detail. It can blur sharp boundaries and local spikes in burglaries that the count map can show more accurately and precisely. Small but intense hotspots may appear diluted, and areas with no burglaries can still show some density simply because they sit near busier cells. KDE is useful for understanding larger, regional patterns, but not as accurate at pinpointing exact locations on a smaller scale.

# Part 4: Create Spatial Predictor Variables

Now we'll create features that might help predict burglaries. We'll use "broken windows theory" logic: signs of disorder predict crime.

## Exercise 4.1: Load 311 Graffiti Removal Calls

```{r load-graffiti, message=FALSE, warning=FALSE, results='hide'}
#| message: false

#Graffiti Removal Requests 
graffiti <- st_read(
  "https://data.cityofchicago.org/api/geospatial/hec5-y4x5?method=export&format=GeoJSON"
) %>%
  st_transform('ESRI:102271')

graffiti_with_units <- graffiti %>%
  st_join(policeDistricts, join = st_within) %>%
  st_join(policeBeats, join = st_within) %>%
  filter(st_is_valid(geometry)) %>%
  filter(!st_is_empty(geometry))

```

## Exercise 4.2: Count of Graffiti Calls per Cell

```{r count-graffiti, message=FALSE, warning=FALSE, results='hide'}

# Aggregate graffiti calls to fishnet
graffiti_fishnet <- st_join(graffiti_with_units, fishnet, join = st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID) %>%
  summarize(graffiti_n = n())   # <-- use a different name here

cat("Rows in graffiti_fishnet:", nrow(graffiti_fishnet), "\n")
cat("Unique cells with any graffiti:", length(unique(graffiti_fishnet$uniqueID)), "\n")

# Join counts back to fishnet
fishnet <- fishnet %>%
  left_join(graffiti_fishnet, by = "uniqueID")

# Replace NAs with 0 (do it outside mutate to avoid masking issues)
fishnet$graffiti_n[is.na(fishnet$graffiti_n)] <- 0

summary(fishnet$graffiti_n)
```

```{r visualize-graffiti, message=FALSE, warning=FALSE}
p1 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = graffiti_n), color = NA) +
  scale_fill_viridis_c(name = "Count", option = "magma") +
  labs(title = "Graffiti Removal 311 Calls") +
  theme_crime()

p2 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +
  scale_fill_viridis_c(name = "Count", option = "plasma") +
  labs(title = "Burglaries") +
  theme_crime()

p1 + p2 +
  plot_annotation(title = "Are graffiti calls and burglaries correlated?")
```

**Question 4.1:** Do you see a visual relationship between graffiti and burglaries? What does this suggest?

There is a small visual overlap between the two maps, with some areas with higher graffiti calls also showing more burglaries, but the patterns are not identical. The graffiti map shows strong clusters, but they depend heavily on who actually reports graffiti issues to 311. Some neighborhoods may know more about 311 calls than others, trust city service employees more, and expect a response tend to report more than those that don't. This means that neighborhoods with high volumes of graffiti 311 calls don't necessarily have the highest volume of graffiti, it just means they have the highest number of people reporting it. The burglary patterns reflects where crimes actually happened, not just where they were reported, showing that burglaries tend to happen based on opportunity, foot traffic, housing density, and policing - not who decides to make a call to 311. The graffiti map shows crime and differences in civil engagement across neighborhoods, while the burglary map reflects crime patterns more accurately. Because of this difference, graffiti calls to 311 alone should not be treated as an accurate measure of crime without considering who reports and who does not report.

## Exercise 4.3: Nearest Neighbor Features

Count in a cell is one measure. Distance to the nearest 3 abandoned cars captures local context.

```{r nn-feature}
#| message: false

# Get coordinates for cell centroids and graffiti points
fishnet_coords <- st_coordinates(st_centroid(fishnet))
graffiti_coords <- st_coordinates(graffiti_with_units)

# Calculate k nearest neighbors and distances (k = 3)
nn_result <- get.knnx(graffiti_coords, fishnet_coords, k = 3)

# Add mean distance to 3 nearest graffiti calls to fishnet
fishnet <- fishnet %>%
  mutate(
    graffiti_with_units.nn = rowMeans(nn_result$nn.dist)
  )

summary(fishnet$graffiti_with_units.nn)

```

**Question 4.2:** What does a low value of `graffiti_with_units.nn` mean? A high value? Why might this be informative?

A low K Nearest Neighbors (KNN) value means the cell is very close to recent graffiti, and a high KNN value means the nearest graffiti is far away, showing how close each area is to graffiti activity. The average distance from each grid cell to its three nearest graffiti incidents is about 109 meters, with distances ranging from roughly half a meter to about 1.45 kilometers. This means some parts of the city are right next to graffiti activity, while others are much farther away. These distances help show how closely each area is situated to places where graffiti regularly occurs.

## Exercise 4.4: Distance to Hot Spots

Let's identify clusters of graffiti calls using Local Moran's I, then calculate distance to these hot spots.

```{r local-morans-abandoned}
# Function to calculate Local Moran's I
calculate_local_morans <- function(data, variable, k = 5) {

  # Use a clean numeric vector (no NAs)
  x <- data[[variable]]
  x[is.na(x)] <- 0

  # Create spatial weights based on k-nearest neighbors
  coords <- st_coordinates(st_centroid(data))
  neighbors <- knn2nb(knearneigh(coords, k = k))
  weights <- nb2listw(neighbors, style = "W", zero.policy = TRUE)

  # Local Moran's I
  local_moran <- localmoran(x, weights)

  mean_val <- mean(x, na.rm = TRUE)

  data %>%
    mutate(
      local_i       = local_moran[, 1],
      p_value       = local_moran[, 5],
      is_significant = p_value < 0.05,
      moran_class   = case_when(
        !is_significant ~ "Not Significant",
        local_i > 0 & .data[[variable]] >  mean_val ~ "High-High",
        local_i > 0 & .data[[variable]] <= mean_val ~ "Low-Low",
        local_i < 0 & .data[[variable]] >  mean_val ~ "High-Low",
        local_i < 0 & .data[[variable]] <= mean_val ~ "Low-High",
        TRUE ~ "Not Significant"
      )
    )
}

# Apply to graffiti counts
fishnet <- calculate_local_morans(fishnet, "graffiti_n", k = 5)
```

```{r visualize-morans}
#| fig-width: 8
#| fig-height: 6

# Visualize hot spots
ggplot() +
  geom_sf(data = fishnet, aes(fill = moran_class), color = NA) +
  scale_fill_manual(
    values = c(
      "High-High"      = "#d7191c",
      "High-Low"       = "#fdae61",
      "Low-High"       = "#abd9e9",
      "Low-Low"        = "#2c7bb6",
      "Not Significant" = "gray90"
    ),
    name = "Cluster Type"
  ) +
  labs(
    title = "Local Moran's I: Graffiti Clusters",
    subtitle = "High-High = Hot spots of disorder"
  ) +
  theme_crime()

```

```{r distance-to-hotspots}
# Get centroids of "High-High" cells (hot spots)
hotspots <- fishnet %>%
  filter(moran_class == "High-High") %>%
  st_centroid()

if (nrow(hotspots) > 0) {

  fishnet <- fishnet %>%
    mutate(
      dist_to_hotspot = as.numeric(
        st_distance(st_centroid(fishnet), hotspots %>% st_union())
      )
    )

  cat("  - Number of hot spot cells:", nrow(hotspots), "\n")

} else {

  fishnet <- fishnet %>%
    mutate(dist_to_hotspot = 0)

}
```

**Question 4.3:** Why might distance to a cluster of abandoned cars be more informative than distance to a graffiti call? What does Local Moran's I tell us?

A Local Moran’s I statistic was used to identify spatial clusters of graffiti activity across Chicago. Local Moran's I does not measure the distance to individual graffiti calls like the KNN does, but instead examines how similar each cell is to it's neighbor. Using five nearest neighbors, the analysis identified 255 "high-high" hot-spot cells, meaning these grid cells have high graffiti counts and are surrounded by other high-count cells. These represent the strongest concentrations of graffiti activity in the city. Only a small number of "low-high" outliers appeared, cells with low graffiti counts located next to areas of high activity. Overall, the results show that graffiti is not evenly spread out. Instead, it forms district clusters and hot-spots, indicating concentrated graffiti activity rather than random distribution.

------------------------------------------------------------------------

# Part 5: Join Police Districts for Cross-Validation

We'll use police districts for our spatial cross-validation.

```{r join-districts}
# Join district information to fishnet
# Make sure policeDistricts has a 'District' column
if (!"District" %in% names(policeDistricts)) {
  policeDistricts <- policeDistricts %>%
    dplyr::rename(District = dist_num)
}

# Join district information to fishnet
fishnet <- st_join(
  fishnet,
  policeDistricts,
  join = st_within,
  left = TRUE
)

# Drop cells outside any district
fishnet <- fishnet %>%
  filter(!is.na(District))

# Now create modeling data
fishnet_model <- fishnet %>%
  st_drop_geometry() %>%
  dplyr::select(
    uniqueID,
    District,
    countBurglaries,
    graffiti_n,              
    graffiti_with_units.nn,
    dist_to_hotspot
  ) %>%
  na.omit()

summary_table <- tibble(
  total_cells             = nrow(fishnet),
  zero_burglary_cells     = sum(fishnet$countBurglaries == 0),
  percent_zero            = round(mean(fishnet$countBurglaries == 0) * 100, 1),
  num_districts           = length(unique(fishnet$District)),
  model_observations      = nrow(fishnet_model),
  model_variables         = ncol(fishnet_model)
)

knitr::kable(summary_table, caption = "Summary of Data Preparation")
```

# Part 6: Model Fitting

## Exercise 6.1: Poisson Regression

Burglary counts are count data (0, 1, 2, 3...). We'll use **Poisson regression**.

```{r fit-poisson, message=FALSE, warning=FALSE, results='hide'}
# Fit Poisson regression
model_poisson <- glm(
  countBurglaries ~ graffiti_n + graffiti_with_units.nn + dist_to_hotspot,
  data  = fishnet_model,
  family = "poisson"
)

model_summary <- broom::tidy(model_poisson) %>%
  mutate(across(where(is.numeric), ~ round(., 4)))

knitr::kable(
  model_summary,
  caption = "Poisson Regression Model Summary"
)
```

**Question 6.1:** Interpret the coefficients. Which variables are significant? What do the signs (positive/negative) tell you?

The model shows that all three predictors are statistically significant. The coefficient for graffiti_n is 0.000072, meaning that as the number of graffiti calls in a cell increases, burglary counts increase slightly. The dist_to_hotspot variable has a small positive coefficient of 0.000012, meaning burglary counts rise slightly as a cell gets farther from major graffiti clusters. The coefficient for graffiti_with_units.nn is –0.00633, so cells that are closer to recent graffiti incidents (with a lower KNN distance) tend to have more burglaries; the negative sign reflects that shorter distances correspond to higher risk. Overall, the results suggest that immediate, local graffiti activity is more closely related to burglary patterns than overall distance to broader graffiti hot-spots.

## Exercise 6.2: Check for Overdispersion

Poisson regression assumes mean = variance. Real count data often violates this (overdispersion).

```{r check-overdispersion}
# Calculate dispersion parameter
dispersion <- sum(residuals(model_poisson, type = "pearson")^2) /
              model_poisson$df.residual

tibble(
  dispersion = round(dispersion, 2),
  overdispersed = ifelse(dispersion > 1.5, "Yes (use Negative Binomial)", "No (Poisson OK)")
) |>
  knitr::kable(caption = "Overdispersion Check")

```

## Exercise 6.3: Negative Binomial Regression

If overdispersed, use **Negative Binomial regression** (more flexible).

```{r fit-negbin}
# Fit Negative Binomial model
model_nb <- glm.nb(
  countBurglaries ~ graffiti_n + graffiti_with_units.nn + dist_to_hotspot,
  data = fishnet_model
)

# Save AIC
nb_aic <- AIC(model_nb)

# Create a clean coefficient table
tidy(model_nb) |>
  mutate(
    estimate  = round(estimate, 4),
    std.error = round(std.error, 4),
    statistic = round(statistic, 2),
    p.value   = round(p.value, 4)
  ) |>
  kable(caption = "Negative Binomial Regression Coefficients")

comparison <- tibble(
  Model = c("Poisson", "Negative Binomial"),
  AIC   = c(AIC(model_poisson), AIC(model_nb))
) |>
  mutate(AIC = round(AIC, 1))

kable(comparison, caption = "Model Comparison by AIC")
```

**Question 6.2:** Which model fits better (lower AIC)? What does this tell you about the data?

AIC (Akaike Information Criterion) is a common measure used to compare statistical models, it evaluates model quality, and a lower AIC value indicates a better-fitting, more efficient model. Here, we are comparing the Poisson Regression Model to the Negative Binomial Regression Model, using the same predicton.

The Negative Binomial model fits the data much better than the Poisson model because it has a much lower AIC value (7654.1 compared to 9456.1). This large difference indicates that the Poisson model is not suitable for the data. The dispersion parameter was estimated at 3.61, which is far above the rule-of-thumb cutoff of 1.5 and shows strong overdispersion in the burglary counts. Because the Poisson model assumes that the mean and variance are equal, it cannot handle this extra variability. The Negative Binomial model includes an additional parameter to model that extra variance, which is why its AIC is much lower and why it provides a more reliable fit for the burglary data.

# Part 7: Spatial Cross-Validation

Standard cross-validation randomly splits data. But with spatial data, this means training on cells right next to test cells (information leakage!).

**Leave-One-Group-Out (LOGO) Cross-Validation** trains on all districts except one, then tests on the held-out district.

```{r spatial-cv}
#| code-fold: true
#| message: false
#| warning: false
#| results: "hide"

# Get unique districts
districts   <- unique(fishnet_model$District)
cv_results  <- tibble()


for (i in seq_along(districts)) {
  
  test_district <- districts[i]
  
  train_data <- fishnet_model %>% filter(District != test_district)
  test_data  <- fishnet_model %>% filter(District == test_district)
  
 model_cv <- glm.nb(
  countBurglaries ~ graffiti_n + graffiti_with_units.nn + dist_to_hotspot,
  data = train_data
)
  
  test_data <- test_data %>%
    mutate(
      prediction = predict(model_cv, newdata = test_data, type = "response")
    )
  
  mae  <- mean(abs(test_data$countBurglaries - test_data$prediction))
  rmse <- sqrt(mean((test_data$countBurglaries - test_data$prediction)^2))
  
  
mean_mae  <- round(mean(cv_results$mae), 2)
mean_rmse <- round(mean(cv_results$rmse), 2)

tibble(
  Metric = c("Mean MAE", "Mean RMSE"),
  Value  = c(mean_mae, mean_rmse)
) %>%
  kable(
    caption = "Cross-Validation Error Summary",
    digits = 2
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
  
  cv_results <- bind_rows(
    cv_results,
    tibble(
      fold         = i,
      test_district = test_district,
      n_test       = nrow(test_data),
      mae          = mae,
      rmse         = rmse
    )
  )
}
cv_results %>%
  arrange(desc(mae)) %>%
  kable(
    digits  = 2,
    caption = "LOGO CV Results by District"
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

```

**Question 7.1:** Why is spatial CV more appropriate than random CV for this problem? Which districts were hardest to predict?

Spatial cross-validation is more appropriate than random CV because crime data is spatially clustered, meaning nearby cells tend to share similar burglary and graffiti patterns. If the data were split randomly, many training cells would be located right next to test cells, allowing information to leak across space and making the model appear more accurate than it really is. Holding out an entire district at a time avoids this problem by forcing the model to predict in areas it has not seen before. Based on the results, District 3 (fold 7) was the hardest to predict, with the highest errors, including a MAE of about 6.31 and an RMSE of about 8.42. This suggests that District 3 behaves differently from the rest of the city. Areas such as District 19 and District 22 were among the easiest to predict, with much lower error values.

The model only predicts well in areas that are already heavily policed, meaning it is learning policing patterns rather than true crime patterns. This limits its usefulness because it will reinforce existing biases and fail to generalize to areas with less police presence or reporting.

# Part 8: Model Predictions and Comparison

## Exercise 8.1: Generate Final Predictions

```{r final-predictions}
# Fit final model on all data
final_model <- glm.nb(
  countBurglaries ~ graffiti_n + graffiti_with_units.nn + dist_to_hotspot,
  data = fishnet_model
)

fishnet <- fishnet %>%
  mutate(
    prediction_nb = predict(
      final_model,
      newdata = fishnet_model,
      type = "response"
    )[match(uniqueID, fishnet_model$uniqueID)]
  )

kde_sum   <- sum(fishnet$kde_value,        na.rm = TRUE)
count_sum <- sum(fishnet$countBurglaries,  na.rm = TRUE)

fishnet <- fishnet %>%
  mutate(
    prediction_kde = (kde_value / kde_sum) * count_sum
  )
```

## Exercise 8.2: Compare Model vs. KDE Baseline

```{r compare-models}
#| fig-width: 12
#| fig-height: 4

# Create three maps
#| fig-width: 12
#| fig-height: 4

p1 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +
  scale_fill_viridis_c(name = "Count", option = "plasma", limits = c(0, 15)) +
  labs(title = "Actual Burglaries") +
  theme_crime()

p2 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = prediction_nb), color = NA) +
  scale_fill_viridis_c(name = "Predicted", option = "plasma", limits = c(0, 15)) +
  labs(title = "Model Predictions (Neg. Binomial)") +
  theme_crime()

p3 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = prediction_kde), color = NA) +
  scale_fill_viridis_c(name = "Predicted", option = "plasma", limits = c(0, 15)) +
  labs(title = "KDE Baseline Predictions") +
  theme_crime()

p1 + p2 + p3 +
  plot_annotation(
    title    = "Actual vs. Predicted Burglaries",
    subtitle = "Does our complex model outperform simple KDE?"
  )
```

```{r model-comparison-metrics}
# Calculate performance metrics
comparison <- fishnet %>%
  st_drop_geometry() %>%
  filter(!is.na(prediction_nb), !is.na(prediction_kde)) %>%
  summarize(
    model_mae  = mean(abs(countBurglaries - prediction_nb)),
    model_rmse = sqrt(mean((countBurglaries - prediction_nb)^2)),
    kde_mae    = mean(abs(countBurglaries - prediction_kde)),
    kde_rmse   = sqrt(mean((countBurglaries - prediction_kde)^2))
  )

comparison %>%
  pivot_longer(everything(), names_to = "metric", values_to = "value") %>%
  separate(metric, into = c("approach", "metric"), sep = "_") %>%
  pivot_wider(names_from = metric, values_from = value) %>%
  kable(
    digits  = 2,
    caption = "Model Performance Comparison"
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

**Question 8.1:** Does the complex model outperform the simple KDE baseline? By how much? Is the added complexity worth it?

The complex model does not outperform the simple KDE baseline. The KDEpredicts burglaries more accurately, with a lower MAE (2.06 vs. 2.56), and a lower RMSE (2.95 vs. 3.66). This means the KDE is consistently closer to the true burglary counts and makes a smaller amount of large errors. Because the KDE is more simple and more accurate, the added complexity of the regression model is not worth it because the baseline model already captures the main spatial pattern better than the more complicated approach.

## Exercise 9.3: Where Does the Model Work Well?

```{r prediction-errors}
#| fig-width: 10
#| fig-height: 5

fishnet <- fishnet %>%
  mutate(
    error_nb      = countBurglaries - prediction_nb,
    error_kde     = countBurglaries - prediction_kde,
    abs_error_nb  = abs(error_nb),
    abs_error_kde = abs(error_kde)
  )

p1 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = error_nb), color = NA) +
  scale_fill_gradient2(
    name     = "Error",
    low      = "#2166ac",
    mid      = "white",
    high     = "#b2182b",
    midpoint = 0,
    limits   = c(-10, 10)
  ) +
  labs(title = "Model Errors (Actual - Predicted)") +
  theme_crime()

p2 <- ggplot() +
  geom_sf(data = fishnet, aes(fill = abs_error_nb), color = NA) +
  scale_fill_viridis_c(name = "Abs. Error", option = "magma") +
  labs(title = "Absolute Model Errors") +
  theme_crime()

p1 + p2

```

**Question 9.2:** Where does the model make the biggest errors? Are there spatial patterns in the errors? What might this reveal?

The final Negative Binomial model shows that only the KNN graffiti distance and the distance to the hot-spot are meaningful predictors of burglary. The KNN variable (rate ratio 0.992) is strongly significant, meaning burglaries are more likely in places that are very close to recent graffiti incidents. Distance to the larger hot-spot is also significant, but the effect is very mall. The simple graffiti count variable is not significant, meaning the number of graffiti calls in a cell does not explain burglary once distance-based measures are included. Overall, the results suggest that proximity to recent graffiti activity is the most useful predictor of burglary risk, more than the total amount of graffiti.

# Part 10: Summary Statistics and Tables

## Exercise 10.1: Model Summary Table

```{r model-summary-table}
# Create nice summary table
model_summary <- broom::tidy(final_model, exponentiate = TRUE) %>%
  mutate(
    across(where(is.numeric), ~round(., 3))
  )

model_summary %>%
  kable(
    caption   = "Final Negative Binomial Model Coefficients (Exponentiated)",
    col.names = c("Variable", "Rate Ratio", "Std. Error", "Z", "P-Value")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover")) %>%
  footnote(
    general = "Rate ratios > 1 indicate positive association with burglary counts."
  )
```

## Exercise 10.2: Key Findings Summary

**Technical Performance:**

-   Cross-validation MAE: `r round(mean(cv_results$mae), 2)`
-   Model vs. KDE: The KDE baseline performed better than the regression model
-   Most predictive variable: The nearest-neighbor graffiti distance (graffiti_with_units.nn)

Overall, the model’s technical performance was moderate, as shown by its cross-validation MAE. However, the simple KDE approach still predicted burglaries more accurately than the regression model. While raw 311 graffiti counts were not reliable because some neighborhoods under report issues, the distance to the closest graffiti incidents captured meaningful patterns of physical disorder. Areas located near recent graffiti tended to have higher burglary risk, even if the total number of 311 calls wasn’t a trustworthy measure on its own.

**Spatial Patterns:**

Burglaries in Chicago are not spread out evenly across the city. Instead, they cluster in specific neighborhoods, especially in busy and densely populated areas. The West Side, the Near North and Near West, and parts of the South Side showed the highest concentrations of crime. These areas higher density than the areas with no clusters, with more residential, commercial and foot traffic. 

Model residuals were not spatially random, they showed systematic clustering. It consistently underestimated crime in neighborhoods where burglaries were common and overestimated crime in low-crime areas.This is because the model was missing important factors that contribute to how burglary occurs in space. Neighborhood dynamics, such as differences in how often residents report issues to 311, variations in housing density, levels of foot traffic, and other social or environmental conditions were not included in the model. Without these key factors, the model wasn’t able to fully capture the true spatial drivers of burglary, leading to prediction errors. This tell us that burglary is shaped by broader neighborhood conditions. 

**Model Limitations:**

The burglary data was more complicated than what a basic Poisson model could handle. The number of burglaries varies much more across the city than the Poisson model expects, so it can’t accurately represent what’s happening on the ground. Because of that extra variation, we had to use a Negative Binomial model, which is designed to deal with messier, more uneven data.

Even after switching models, the errors still showed strong spatial patterns. Certain parts of the city consistently behaved differently than the model predicted, which suggests that important location-based factors were still missing from the analysis. Things like neighborhood characteristics, population density, foot traffic, and differences in how communities interact with city services all play a role in shaping where burglaries happen. For example, 311 graffiti calls don’t represent the actual amount of graffiti everywhere—they reflect who chooses to report issues. Some neighborhoods trust the 311 system more, expect a quicker response, or are more familiar with the service, while others may under report. As a result, the graffiti data captures differences in civic engagement as much as it captures actual physical disorder. This uneven reporting introduces bias and makes the graffiti variables less reliable as predictors.

Another challenge is that about 24% of all grid cells had zero burglaries. When so many areas have no incidents at all, it becomes even harder for a statistical model to learn meaningful patterns, because the data is very zero-heavy and uneven.

When you combine all of these issues—extra variation in burglary counts, spatial patterns that weren’t explained by the predictors, and the uneven reporting of 311 calls, it becomes clear why the simple KDE baseline performed better. KDE naturally emphasizes clustering and doesn’t rely on strict statistical assumptions, so it ends up capturing the overall spatial pattern of burglaries more effectively than the more complicated regression model.

# Conclusion and Next Steps

This analysis showed that burglaries in Chicago are highly clustered and that simple spatial smoothing (KDE) can outperform more complex regression models when important neighborhood factors are missing. The regression model struggled with over dispersion, uneven 311 reporting, and many zero-crime areas, all of which made prediction harder.

Moving forward, the model could be improved by adding stronger spatial and social predictors, such as population density, land use, and neighborhood characteristics, and by using disorder measures that aren’t affected by reporting biases.

::::::::::
