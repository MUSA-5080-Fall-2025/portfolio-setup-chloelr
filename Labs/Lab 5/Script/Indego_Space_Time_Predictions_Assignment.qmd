---
title: "Space-Time Prediction of Bike Share Demand: Philadelphia Indego"
author: "Chloe Robinson"
date: "`r Sys.Date()`"
code-fold: false
code-tools: false
format:
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    theme: cosmo
    embed-resources: true
editor: visual
execute:
  warning: false
  message: false
  freeze: false     
  cache: false      
---

```{r}
#| echo: false
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE
)
```

# Introduction

## The Rebalancing Challenge in Philadelphia

Philadelphia's Indego bike share system faces the same operational challenge as every bike share system: **rebalancing bikes to meet anticipated demand**.

Imagine you're an Indego operations manager at 6:00 AM on a Monday morning. You have: - 200 stations across Philadelphia - Limited trucks and staff for moving bikes - 2-3 hours before morning rush hour demand peaks - **The question:** Which stations will run out of bikes by 8:30 AM?

This lab will teach you to build predictive models that forecast bike share demand across **space** (different stations) and **time** (different hours) to help solve this operational problem.

------------------------------------------------------------------------

```{r}
#| echo: false
# Core tidyverse
library(tidyverse)
library(lubridate)

# Spatial data
library(sf)
library(tigris)

# Census data
library(tidycensus)

# Weather data
library(riem)  # For Philadelphia weather from ASOS stations


# Visualization
library(viridis)
library(gridExtra)
library(knitr)
library(kableExtra)
library(zoo)

# here!
library(here)
# Get rid of scientific notation. We gotta look good!
options(scipen = 999)
```

```{r}
#| echo: false
plotTheme <- theme(
  plot.title = element_text(size = 14, face = "bold"),
  plot.subtitle = element_text(size = 10),
  plot.caption = element_text(size = 8),
  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
  axis.text.y = element_text(size = 10),
  axis.title = element_text(size = 11, face = "bold"),
  panel.background = element_blank(),
  panel.grid.major = element_line(colour = "#D0D0D0", size = 0.2),
  panel.grid.minor = element_blank(),
  axis.ticks = element_blank(),
  legend.position = "right"
)

mapTheme <- theme(
  plot.title = element_text(size = 14, face = "bold"),
  plot.subtitle = element_text(size = 10),
  plot.caption = element_text(size = 8),
  axis.line = element_blank(),
  axis.text = element_blank(),
  axis.ticks = element_blank(),
  axis.title = element_blank(),
  panel.background = element_blank(),
  panel.border = element_blank(),
  panel.grid.major = element_line(colour = 'transparent'),
  panel.grid.minor = element_blank(),
  legend.position = "right",
  plot.margin = margin(1, 1, 1, 1, 'cm'),
  legend.key.height = unit(1, "cm"),
  legend.key.width = unit(0.2, "cm")
)

palette5 <- c("#eff3ff", "#bdd7e7", "#6baed6", "#3182bd", "#08519c")
```

# Data Import & Preparation

## Load Indego Trip Data (Q1 2025)

```{r}
#| echo: false
#| message: false 
#| output: false

# Read Q1 2025 data
indego_q1 <- read_csv(
  here("C:/Users/chloe.robinson/Desktop/PENN/FALL 2025/Public Policy Analytics/Week 1/portfolio-setup-chloelr/Labs/Lab 5/Data/indego-trips-2025-q1.csv")
)

# Quick look at the data
glimpse(indego_q1)
```

```{r}
#| echo: false
#| results: hide
#| message: false
#| warning: false

# How many trips?
cat("Total trips in Q1 2025:", nrow(indego_q1), "\n")

# Date range
cat("Date range:",
    min(mdy_hm(indego_q1$start_time)), "to",
    max(mdy_hm(indego_q1$start_time)), "\n")

# How many unique stations?
cat("Unique start stations:", length(unique(indego_q1$start_station)), "\n")

# Trip types
table(indego_q1$trip_route_category)

# Passholder types
table(indego_q1$passholder_type)

# Bike types
table(indego_q1$bike_type)
```

```{r summary_table_q1}
#| echo: false
#| message: false
#| warning: false

summary_table_q1 <- tibble(
  `Total Trips`           = nrow(indego_q1),
  `Date Range Start`      = min(mdy_hm(indego_q1$start_time)),
  `Date Range End`        = max(mdy_hm(indego_q1$start_time)),
  `Unique Start Stations` = length(unique(indego_q1$start_station))
)

kable(summary_table_q1, caption = "Summary of Indego Trip Data – Q1 2025") |>
  kable_styling(latex_options = "hold_position")
```

## Create Time Bins (Q1 2025)

```{r}
indego_q1 <- indego_q1 %>%
  mutate(
    # Parse datetime
    start_datetime = mdy_hm(start_time),
    end_datetime   = mdy_hm(end_time),

    # Create hourly bins
    interval60 = floor_date(start_datetime, unit = "hour"),

    # Extract time features
    week  = week(interval60),
    month = month(interval60, label = TRUE),
    dotw  = wday(interval60, label = TRUE),
    hour  = hour(interval60),
    date  = as.Date(interval60),

    # Create useful indicators
    weekend   = ifelse(dotw %in% c("Sat", "Sun"), 1, 0),
    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)
  )
```

```{r}
#| echo: false

indego_q1 %>%
  select(start_datetime, interval60, week, dotw, hour, weekend) %>%
  head(10) %>%
  kable(caption = "Preview of temporal features – Q1 2025") %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

# Exploratory Analysis Q1 2025

## Trips Over Time (Q1 2025)

```{r}
# Daily trip counts
daily_trips_q1 <- indego_q1 %>%
  group_by(date) %>%
  summarize(trips = n())

ggplot(daily_trips_q1, aes(x = date, y = trips)) +
  geom_line(color = "#3182bd", linewidth = 1) +
  geom_smooth(se = FALSE, color = "red", linetype = "dashed") +
  labs(
    title = "Indego Daily Ridership – Q1 2025",
    subtitle = "Winter ridership patterns in Philadelphia",
    x = "Date",
    y = "Daily Trips",
    caption = "Source: Indego bike share"
  ) +
  plotTheme
```

## Hourly Patterns (Q1 2025)

```{r}
hourly_patterns_q1 <- indego_q1 %>%
  group_by(hour, weekend) %>%
  summarize(avg_trips = n() / n_distinct(date)) %>%
  mutate(day_type = ifelse(weekend == 1, "Weekend", "Weekday"))

ggplot(hourly_patterns_q1, aes(x = hour, y = avg_trips, color = day_type)) +
  geom_line(linewidth = 1.2) +
  scale_color_manual(values = c("Weekday" = "#08519c", "Weekend" = "#6baed6")) +
  labs(
    title = "Average Hourly Ridership Patterns – Q1 2025",
    subtitle = "Weekday commute peaks vs. weekend leisure patterns",
    x = "Hour of Day",
    y = "Average Trips per Hour",
    color = "Day Type"
  ) +
  plotTheme
```

## Top Stations (Q1 2025)

```{r}
top_stations_q1 <- indego_q1 %>%
  count(start_station, start_lat, start_lon, name = "trips") %>%
  arrange(desc(trips)) %>%
  head(10)
```

```{r}
#| echo: false

kable(top_stations_q1,
      caption = "Top 10 Indego Stations by Trip Origins – Q1 2025",
      format.args = list(big.mark = ",")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

# Get Philadelphia Spatial Context (Q1 2025)

## Load Philadelphia Census Data(Q1 2025)

```{r load_census_q1}
#| echo: false
#| results: hide
#| message: false
#| warning: false

philly_census_q1 <- get_acs(
  geography = "tract",
  variables = c(
    "B01003_001",
    "B19013_001",
    "B08301_001",
    "B08301_010",
    "B02001_002",
    "B25077_001"
  ),
  state = "PA",
  county = "Philadelphia",
  year = 2022,
  geometry = TRUE,
  output = "wide"
) %>%
  rename(
    Total_Pop = B01003_001E,
    Med_Inc = B19013_001E,
    Total_Commuters = B08301_001E,
    Transit_Commuters = B08301_010E,
    White_Pop = B02001_002E,
    Med_Home_Value = B25077_001E
  ) %>%
  mutate(
    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,
    Percent_White = (White_Pop / Total_Pop) * 100
  ) %>%
  st_transform(crs = 4326)

glimpse(philly_census_q1)
```

## Map Philadelphia Context (Q1 2025)

```{r map_philly_q1}
ggplot() +
  geom_sf(data = philly_census_q1, aes(fill = Med_Inc), color = NA) +
  scale_fill_viridis(
    option = "viridis",
    name = "Median\nIncome",
    labels = scales::dollar
  ) +
  labs(
    title = "Philadelphia Median Household Income by Census Tract (Q1 2025)",
    subtitle = "Context for bike share demand"
  ) +
  geom_point(
    data = indego_q1,
    aes(x = start_lon, y = start_lat),
    color = "red", size = 0.25, alpha = 0.6
  ) +
  mapTheme
```

## Join Census Data to Stations

```{r join_census_to_stations_q1}
stations_sf_q1 <- indego_q1 %>%
  distinct(start_station, start_lat, start_lon) %>%
  filter(!is.na(start_lat), !is.na(start_lon)) %>%
  st_as_sf(coords = c("start_lon", "start_lat"), crs = 4326)

stations_census_q1 <- st_join(stations_sf_q1, philly_census_q1, left = TRUE) %>%
  st_drop_geometry()

stations_for_map_q1 <- indego_q1 %>%
  distinct(start_station, start_lat, start_lon) %>%
  filter(!is.na(start_lat), !is.na(start_lon)) %>%
  left_join(
    stations_census_q1 %>% select(start_station, Med_Inc),
    by = "start_station"
  ) %>%
  mutate(has_census = !is.na(Med_Inc))

indego_census_q1 <- indego_q1 %>%
  left_join(
    stations_census_q1 %>% 
      select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop),
      by = "start_station"
  )
```

```{r join_census_to_stations2_q1}
#| echo: false
stations_for_map_q1 <- indego_q1 %>%
  distinct(start_station, start_lat, start_lon) %>%
  filter(!is.na(start_lat), !is.na(start_lon)) %>%
  left_join(
    stations_census_q1 %>% select(start_station, Med_Inc),
    by = "start_station"
  ) %>%
  mutate(has_census = !is.na(Med_Inc))

ggplot() +
  geom_sf(data = philly_census_q1, aes(fill = Med_Inc), color = "white", size = 0.1) +
  scale_fill_viridis(
    option = "viridis",
    name = "Median\nIncome",
    labels = scales::dollar,
    na.value = "grey90"
  ) +
  geom_point(
    data = stations_for_map_q1 %>% filter(has_census),
    aes(x = start_lon, y = start_lat),
    color = "grey30", size = 1, alpha = 0.6
  ) +
  geom_point(
    data = stations_for_map_q1 %>% filter(!has_census),
    aes(x = start_lon, y = start_lat),
    color = "red", size = 1, shape = 4, stroke = 1.5
  ) +
  labs(
    title = "Philadelphia Median Household Income (Q1 2025)",
    subtitle = "RED = stations without census match",
    caption = "Non-residential or outside-tract stations"
  ) +
  mapTheme
```

```{r}
valid_stations_q1 <- stations_census_q1 %>%
  filter(!is.na(Med_Inc)) %>%
  pull(start_station)

indego_census_q1 <- indego_q1 %>%
  filter(start_station %in% valid_stations_q1) %>%
  left_join(
    stations_census_q1 %>% 
      select(start_station, Med_Inc, Percent_Taking_Transit, 
             Percent_White, Total_Pop),
    by = "start_station"
  )
```

# Dealing with missing data (Q1 2025)

## Get Weather Data (Q1 2025)

```{r get_weather_q1}
weather_data_q1 <- riem_measures(
  station    = "PHL",
  date_start = "2025-01-01",
  date_end   = "2025-03-31"
)

weather_processed_q1 <- weather_data_q1 %>%
  mutate(
    interval60    = floor_date(valid, unit = "hour"),
    Temperature   = tmpf,
    Precipitation = ifelse(is.na(p01i), 0, p01i),
    Wind_Speed    = sknt
  ) %>%
  select(interval60, Temperature, Precipitation, Wind_Speed) %>%
  distinct()

weather_complete_q1 <- weather_processed_q1 %>%
  complete(interval60 = seq(min(interval60), max(interval60), by = "hour")) %>%
  fill(Temperature, Precipitation, Wind_Speed, .direction = "down")
```

## Visualize Weather Patterns (Q1 2025)

```{r visualize_weather_q1}
ggplot(weather_complete_q1, aes(x = interval60, y = Temperature)) +
  geom_line(color = "#3182bd", alpha = 0.7) +
  geom_smooth(se = FALSE, color = "red") +
  labs(
    title = "Philadelphia Temperature – Q1 2025",
    subtitle = "Winter conditions",
    x = "Date",
    y = "Temperature (°F)"
  ) +
  plotTheme
```

# Create Space-TIme Panel

## Aggregate Trips to Station-Hour Level (Q1 2025)

```{r aggregate_trips_q1}
trips_panel_q1 <- indego_census_q1 %>%
  group_by(interval60, start_station, start_lat, start_lon,
           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%
  summarize(Trip_Count = n()) %>%
  ungroup()
```

## Create Complete Panel Structure (Q1 2025)

```{r complete_panel_q1}
n_stations_q1 <- length(unique(trips_panel_q1$start_station))
n_hours_q1    <- length(unique(trips_panel_q1$interval60))
expected_rows_q1 <- n_stations_q1 * n_hours_q1

study_panel_q1 <- expand.grid(
  interval60   = unique(trips_panel_q1$interval60),
  start_station = unique(trips_panel_q1$start_station)
) %>%
  left_join(trips_panel_q1, by = c("interval60", "start_station")) %>%
  mutate(Trip_Count = replace_na(Trip_Count, 0))

station_attributes_q1 <- trips_panel_q1 %>%
  group_by(start_station) %>%
  summarize(
    start_lat = first(start_lat),
    start_lon = first(start_lon),
    Med_Inc   = first(Med_Inc),
    Percent_Taking_Transit = first(Percent_Taking_Transit),
    Percent_White = first(Percent_White),
    Total_Pop = first(Total_Pop)
  )

study_panel_q1 <- study_panel_q1 %>%
  left_join(station_attributes_q1, by = "start_station")
```

## Add Time Features (Q1 2025)

```{r add_time_features_q1}
study_panel_q1 <- study_panel_q1 %>%
  mutate(
    week     = week(interval60),
    month    = month(interval60, label = TRUE),
    dotw     = wday(interval60, label = TRUE),
    hour     = hour(interval60),
    date     = as.Date(interval60),
    weekend  = ifelse(dotw %in% c("Sat", "Sun"), 1, 0),
    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)
  )
```

## Join Weather Data (Q1 2025)

```{r join_weather_q1}
study_panel_q1 <- study_panel_q1 %>%
  left_join(weather_complete_q1, by = "interval60")
```

#Create Temporal Lag Variables (Q1 2025)

```{r create_lags_q1}
study_panel_q1 <- study_panel_q1 %>%
  arrange(start_station, interval60)

study_panel_q1 <- study_panel_q1 %>%
  group_by(start_station) %>%
  mutate(
    lag1Hour   = lag(Trip_Count, 1),
    lag2Hours  = lag(Trip_Count, 2),
    lag3Hours  = lag(Trip_Count, 3),
    lag12Hours = lag(Trip_Count, 12),
    lag1day    = lag(Trip_Count, 24)
  ) %>%
  ungroup()

study_panel_complete_q1 <- study_panel_q1 %>%
  filter(!is.na(lag1day))

study_panel_complete_q1 <- study_panel_complete_q1 %>%
  mutate(
    month = factor(month, levels = c("Jan", "Feb", "Mar")),
    dotw  = factor(dotw,  levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"))
  )
```

```{r lag_correlations_q1}
example_station_q1 <- study_panel_complete_q1 %>%
  filter(start_station == first(start_station)) %>%
  head(168)

ggplot(example_station_q1, aes(x = interval60)) +
  geom_line(aes(y = Trip_Count, color = "Current"), linewidth = 1) +
  geom_line(aes(y = lag1Hour, color = "1 Hour Ago"), linewidth = 1, alpha = 0.7) +
  geom_line(aes(y = lag1day, color = "24 Hours Ago"), linewidth = 1, alpha = 0.7) +
  scale_color_manual(values = c(
    "Current"     = "#08519c",
    "1 Hour Ago"  = "#3182bd",
    "24 Hours Ago"= "#6baed6"
  )) +
  labs(
    title  = "Temporal Lag Patterns at One Station – Q1 2025",
    x      = "Date-Time",
    y      = "Trip Count",
    color  = "Time Period"
  ) +
  plotTheme
```

## Temporal Train/Test Split (Q1 2025)

```{r temporal_split_q1}
train_weeks_q1 <- 1:9
test_weeks_q1  <- 10:13

early_stations_q1 <- study_panel_complete_q1 %>%
  filter(week %in% train_weeks_q1, Trip_Count > 0) %>%
  distinct(start_station) %>%
  pull(start_station)

late_stations_q1 <- study_panel_complete_q1 %>%
  filter(week %in% test_weeks_q1, Trip_Count > 0) %>%
  distinct(start_station) %>%
  pull(start_station)

common_stations_q1 <- intersect(early_stations_q1, late_stations_q1)

study_panel_q1_ts <- study_panel_complete_q1 %>%
  filter(start_station %in% common_stations_q1)

train_q1 <- study_panel_q1_ts %>%
  filter(week %in% train_weeks_q1)

test_q1 <- study_panel_q1_ts %>%
  filter(week %in% test_weeks_q1)
```

```{r summary_train_test_q1}
#| echo: false

summary_table_q1_ts <- tibble(
  Set = c("Training (Q1 2025)", "Testing (Q1 2025)"),
  Observations = c(nrow(train_q1), nrow(test_q1)),
  Date_Range = c(
    paste(min(train_q1$date), "to", max(train_q1$date)),
    paste(min(test_q1$date), "to", max(test_q1$date))
  )
)

kable(summary_table_q1_ts,
      caption = "Train/Test Summary – Q1 2025") %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

# Build Predictive Models

## Model 1: Baseline (Time + Weather) — Q1 2025

```{r model1_q1}
train_q1 <- train_q1 %>%
  mutate(dotw_simple = factor(dotw, 
                              levels = c("Mon","Tue","Wed","Thu","Fri","Sat","Sun")))

contrasts(train_q1$dotw_simple) <- contr.treatment(7)

model1_q1 <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple +
    Temperature + Precipitation,
  data = train_q1
)
```

## Model 2: Add Temporal Lags — Q1 2025

```{r model2_q1}
model2_q1 <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple +
    Temperature + Precipitation +
    lag1Hour + lag3Hours + lag1day,
  data = train_q1
)
```

## Model 3: Add Demographics — Q1 2025

```{r model3_q1}
model3_q1 <- lm(
  Trip_Count ~ 
    as.factor(hour) +
    dotw_simple +
    Temperature +
    Precipitation +
    lag1Hour +
    lag3Hours +
    lag1day +
    Med_Inc.x +
    Percent_Taking_Transit.x +
    Percent_White.x,
  data = train_q1
)
```

## Model 4: Add Station Fixed Effects — Q1 2025

```{r model4_q1}
model4_q1 <- lm(
  Trip_Count ~ 
    as.factor(hour) +
    dotw_simple +
    Temperature +
    Precipitation +
    lag1Hour +
    lag3Hours +
    lag1day +
    Med_Inc.x +
    Percent_Taking_Transit.x +
    Percent_White.x +
    as.factor(start_station),
  data = train_q1
)
```

## Model 5: Add Rush Hour Interaction — Q1 2025

```{r model5_q1}
train_q1$month <- factor(train_q1$month, levels = c("Jan","Feb","Mar"))
test_q1$month  <- factor(test_q1$month, levels = levels(train_q1$month))

model5_q1 <- lm(
  Trip_Count ~ 
    as.factor(hour) +
    dotw_simple +
    Temperature +
    Precipitation +
    lag1Hour +
    lag3Hours +
    lag1day +
    rush_hour +
    month +
    Med_Inc.x +
    Percent_Taking_Transit.x +
    Percent_White.x +
    as.factor(start_station) +
    rush_hour * weekend,
  data = train_q1
)
```

# Model Evaluation

## Calculate Predictions and MAE — Q1 2025

```{r calculate_mae_q1}
test_q1$month <- factor(test_q1$month, levels = model5_q1$xlevels$month)

test_q1 <- test_q1 %>%
  mutate(dotw_simple = factor(dotw,
                              levels = c("Mon","Tue","Wed","Thu","Fri","Sat","Sun")))

contrasts(test_q1$dotw_simple) <- contr.treatment(7)

test_q1 <- test_q1 %>%
  mutate(
    pred1_q1 = predict(model1_q1, newdata = test_q1),
    pred2_q1 = predict(model2_q1, newdata = test_q1),
    pred3_q1 = predict(model3_q1, newdata = test_q1),
    pred4_q1 = predict(model4_q1, newdata = test_q1),
    pred5_q1 = predict(model5_q1, newdata = test_q1)
  )

mae_results_q1 <- tibble(
  Model = c(
    "1. Time + Weather",
    "2. + Temporal Lags",
    "3. + Demographics",
    "4. + Station FE",
    "5. + Rush Hour Interaction"
  ),
  MAE = c(
    mean(abs(test_q1$Trip_Count - test_q1$pred1_q1), na.rm = TRUE),
    mean(abs(test_q1$Trip_Count - test_q1$pred2_q1), na.rm = TRUE),
    mean(abs(test_q1$Trip_Count - test_q1$pred3_q1), na.rm = TRUE),
    mean(abs(test_q1$Trip_Count - test_q1$pred4_q1), na.rm = TRUE),
    mean(abs(test_q1$Trip_Count - test_q1$pred5_q1), na.rm = TRUE)
  )
)

kable(mae_results_q1,
      digits = 2,
      caption = "Mean Absolute Error by Model (Q1 2025 Test Set)",
      col.names = c("Model", "MAE (trips)")) %>%
  kable_styling(bootstrap_options = c("striped","hover"))
```

## Visualize Model Comparison — Q1 2025

```{r compare_models_q1}
ggplot(mae_results_q1, aes(x = reorder(Model, -MAE), y = MAE)) +
  geom_col(fill = "#3182bd", alpha = 0.8) +
  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +
  labs(
    title = "Model Performance Comparison — Q1 2025",
    subtitle = "Lower MAE = Better Predictions",
    x = "Model",
    y = "Mean Absolute Error (trips)"
  ) +
  plotTheme +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Space- Time Error Analysis

## Observed vs. Predicted — Q1 2025 (Using Model 2)

```{r obs_vs_pred_q1}
test_q1 <- test_q1 %>%
  mutate(
    error_q1     = Trip_Count - pred2_q1,
    abs_error_q1 = abs(error_q1),
    time_of_day_q1 = case_when(
      hour < 7                  ~ "Overnight",
      hour >= 7 & hour < 10     ~ "AM Rush",
      hour >= 10 & hour < 15    ~ "Mid-Day",
      hour >= 15 & hour <= 18   ~ "PM Rush",
      hour > 18                 ~ "Evening"
    )
  )

ggplot(test_q1, aes(x = Trip_Count, y = pred2_q1)) +
  geom_point(alpha = 0.2, color = "#3182bd") +
  geom_abline(slope = 1, intercept = 0, color = "red", linewidth = 1) +
  geom_smooth(method = "lm", se = FALSE, color = "darkgreen") +
  facet_grid(weekend ~ time_of_day_q1) +
  labs(
    title = "Observed vs. Predicted Trips — Q1 2025",
    x = "Observed Trips",
    y = "Predicted Trips"
  ) +
  plotTheme
```

##Spatial Error Patterns — Q1 2025

```{r spatial_errors_q1}
station_errors_q1 <- test_q1 %>%
  group_by(start_station, start_lat.x, start_lon.x) %>%
  summarize(
    MAE_q1         = mean(abs_error_q1, na.rm = TRUE),
    avg_demand_q1  = mean(Trip_Count, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  filter(!is.na(start_lat.x), !is.na(start_lon.x))

p1_q1 <- ggplot() +
  geom_sf(data = philly_census_q1, fill = "grey95", color = "white", size = 0.2) +
  geom_point(
    data = station_errors_q1,
    aes(x = start_lon.x, y = start_lat.x, color = MAE_q1),
    size = 3.5,
    alpha = 0.7
  ) +
  scale_color_viridis(
    option = "plasma",
    name = "MAE (trips)",
    direction = -1
  ) +
  labs(title = "Prediction Errors — Q1 2025") +
  mapTheme +
  theme(legend.position = "bottom")

p2_q1 <- ggplot() +
  geom_sf(data = philly_census_q1, fill = "grey95", color = "white", size = 0.2) +
  geom_point(
    data = station_errors_q1,
    aes(x = start_lon.x, y = start_lat.x, color = avg_demand_q1),
    size = 3.5,
    alpha = 0.7
  ) +
  scale_color_viridis(
    option = "viridis",
    name = "Avg Demand (trips/hr)",
    direction = -1
  ) +
  labs(title = "Average Demand — Q1 2025") +
  mapTheme +
  theme(legend.position = "bottom")

grid.arrange(p1_q1, p2_q1, ncol = 2)
```

## Temporal Error Patterns — Q1 2025

```{r temporal_errors_q1}
temporal_errors_q1 <- test_q1 %>%
  group_by(time_of_day_q1, weekend) %>%
  summarize(
    MAE_q1 = mean(abs_error_q1, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(day_type_q1 = ifelse(weekend == 1, "Weekend", "Weekday"))

ggplot(temporal_errors_q1, aes(x = time_of_day_q1, y = MAE_q1, fill = day_type_q1)) +
  geom_col(position = "dodge") +
  scale_fill_manual(values = c("Weekday" = "#08519c", "Weekend" = "#6baed6")) +
  labs(
    title = "Prediction Errors by Time of Day — Q1 2025",
    x = "Time of Day",
    y = "Mean Absolute Error (trips)"
  ) +
  plotTheme +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Errors vs. Demographics — Q1 2025

```{r errors_demographics_q1}
station_errors_demo_q1 <- station_errors_q1 %>%
  left_join(
    station_attributes_q1 %>%
      select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),
    by = "start_station"
  ) %>%
  filter(!is.na(Med_Inc))

p1_q1 <- ggplot(station_errors_demo_q1, aes(x = Med_Inc, y = MAE_q1)) +
  geom_point(alpha = 0.5, color = "#3182bd") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  scale_x_continuous(labels = scales::dollar) +
  labs(x = "Median Income", y = "MAE") +
  plotTheme

p2_q1 <- ggplot(station_errors_demo_q1, aes(x = Percent_Taking_Transit, y = MAE_q1)) +
  geom_point(alpha = 0.5, color = "#3182bd") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(x = "% Taking Transit", y = "MAE") +
  plotTheme

p3_q1 <- ggplot(station_errors_demo_q1, aes(x = Percent_White, y = MAE_q1)) +
  geom_point(alpha = 0.5, color = "#3182bd") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(x = "% White", y = "MAE") +
  plotTheme

grid.arrange(p1_q1, p2_q1, p3_q1, ncol = 2)
```

# INDEGO TRIP DATA Q2 2024

## Load Indego Trip Data (Q2 2024)

```{r}
#| echo: false
#| message: false 
#| output: false
# Read Q1 2025 data
indego <- read_csv(here("C:/Users/chloe.robinson/Desktop/PENN/FALL 2025/Public Policy Analytics/Week 1/portfolio-setup-chloelr/Labs/Lab 5/Data/indego-trips-2024-q2.csv"))

# Quick look at the data
glimpse(indego)
```

```{r}
#| echo: false
#| results: hide
#| message: false
#| warning: false
# How many trips?
cat("Total trips in Q2 2024:", nrow(indego), "\n")

# Date range
cat("Date range:", 
    min(mdy_hm(indego$start_time)), "to", 
    max(mdy_hm(indego$start_time)), "\n")

# How many unique stations?
cat("Unique start stations:", length(unique(indego$start_station)), "\n")

# Trip types
table(indego$trip_route_category)

# Passholder types
table(indego$passholder_type)

# Bike types
table(indego$bike_type)
```

```{r summary_table}
#| echo: false
#| message: false
#| warning: false

summary_table <- tibble(
  `Total Trips`           = nrow(indego),
  `Date Range Start`      = min(mdy_hm(indego$start_time)),
  `Date Range End`        = max(mdy_hm(indego$start_time)),
  `Unique Start Stations` = length(unique(indego$start_station))
)

kable(summary_table, caption = "Summary of Indego Trip Data") |>
  kable_styling(latex_options = "hold_position")
```

## Create Time Bins

We need to aggregate trips into hourly intervals for our panel data structure.

```{r}
indego <- indego %>%
  mutate(
    # Parse datetime
    start_datetime = mdy_hm(start_time),
    end_datetime = mdy_hm(end_time),
    
    # Create hourly bins
    interval60 = floor_date(start_datetime, unit = "hour"),
    
    # Extract time features
    week = week(interval60),
    month = month(interval60, label = TRUE),
    dotw = wday(interval60, label = TRUE),
    hour = hour(interval60),
    date = as.Date(interval60),
    
    # Create useful indicators
    weekend = ifelse(dotw %in% c("Sat", "Sun"), 1, 0),
    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)
  )
```

```{r}
#| echo: false
indego %>%
  select(start_datetime, interval60, week, dotw, hour, weekend) %>%
  head(10) %>%
  kable(caption = "Preview of temporal features") %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

------------------------------------------------------------------------

# Exploratory Analysis

## Trips Over Time

```{r}
# Daily trip counts
daily_trips <- indego %>%
  group_by(date) %>%
  summarize(trips = n())

ggplot(daily_trips, aes(x = date, y = trips)) +
  geom_line(color = "#3182bd", linewidth = 1) +
  geom_smooth(se = FALSE, color = "red", linetype = "dashed") +
  labs(
    title = "Indego Daily Ridership - Q2 2024",
    subtitle = "Spring Demand Paters in Philadelphia",
    x = "Date",
    y = "Daily Trips",
    caption = "Source: Indego bike share"
  ) +
  plotTheme
```

Indego ridership increases at a steady pace from April through June, showing a clear upward seasonal trend as the weather improves. Although overall ridership rises over time, there are occasional dips and highs that likely reflect bad weather or special events in Philadelphia.

## Hourly Patterns

```{r hourly_patterns}
# Average trips by hour and day type
hourly_patterns <- indego %>%
  group_by(hour, weekend) %>%
  summarize(avg_trips = n() / n_distinct(date)) %>%
  mutate(day_type = ifelse(weekend == 1, "Weekend", "Weekday"))

ggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +
  geom_line(linewidth = 1.2) +
  scale_color_manual(values = c("Weekday" = "#08519c", "Weekend" = "#6baed6")) +
  labs(
    title = "Average Hourly Ridership Patterns",
    subtitle = "Clear commute patterns on weekdays",
    x = "Hour of Day",
    y = "Average Trips per Hour",
    color = "Day Type"
  ) +
  plotTheme
```

The chart shows a strong correlation between commuting times and ridership during the weekday. There is a clear spike during the morning commute time during the week at 9am, and an even higher spike in ridership during the evening peak commute time, around 5-6pm. In contrast, the weekend pattern does not show sharp ridership peaks, instead has a steady ridership rises later in the morning, stays spread out across the afternoon, and begins to decline at 3pm.

## Top Stations

```{r top_stations}
# Most popular origin stations
top_stations <- indego %>%
  count(start_station, start_lat, start_lon, name = "trips") %>%
  arrange(desc(trips)) %>%
  head(10)
```

```{r top_stations_table}
#| echo: false
kable(top_stations, 
      caption = "Top 10 Indego Stations by Trip Origins",
      format.args = list(big.mark = ",")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

------------------------------------------------------------------------

# Get Philadelphia Spatial Context

## Load Philadelphia Census Data + Map Context

```{r load_census}
#| echo: false
#| results: hide
#| message: false
#| warning: false

# Get Philadelphia census tracts
philly_census <- get_acs(
  geography = "tract",
  variables = c(
    "B01003_001",  # Total population
    "B19013_001",  # Median household income
    "B08301_001",  # Total commuters
    "B08301_010",  # Commute by transit
    "B02001_002",  # White alone
    "B25077_001"   # Median home value
  ),
  state = "PA",
  county = "Philadelphia",
  year = 2022,
  geometry = TRUE,
  output = "wide"
) %>%
  rename(
    Total_Pop = B01003_001E,
    Med_Inc = B19013_001E,
    Total_Commuters = B08301_001E,
    Transit_Commuters = B08301_010E,
    White_Pop = B02001_002E,
    Med_Home_Value = B25077_001E
  ) %>%
  mutate(
    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,
    Percent_White = (White_Pop / Total_Pop) * 100
  ) %>%
  st_transform(crs = 4326)  # WGS84 for lat/lon matching

# Check the data
glimpse(philly_census)
```

## Map Philadelphia Context

```{r map_philly}
# Map median income
ggplot() +
  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +
  scale_fill_viridis(
    option = "viridis",
    name = "Median\nIncome",
    labels = scales::dollar
  ) +
  labs(
    title = "Philadelphia Median Household Income by Census Tract",
    subtitle = "Context for understanding bike share demand patterns"
  ) +
  # Stations 
  geom_point(
    data = indego,
    aes(x = start_lon, y = start_lat),
    color = "red", size = 0.25, alpha = 0.6
  ) +
  mapTheme
```

## Join Census Data to Stations

We'll spatially join census characteristics to each bike station.

```{r join_census_to_stations}
# Create sf object for stations
stations_sf <- indego %>%
  distinct(start_station, start_lat, start_lon) %>%
  filter(!is.na(start_lat), !is.na(start_lon)) %>%
  st_as_sf(coords = c("start_lon", "start_lat"), crs = 4326)

# Spatial join to get census tract for each station
stations_census <- st_join(stations_sf, philly_census, left = TRUE) %>%
  st_drop_geometry()

# Look at the result - investigate whether all of the stations joined to census data

stations_for_map <- indego %>%
  distinct(start_station, start_lat, start_lon) %>%
  filter(!is.na(start_lat), !is.na(start_lon)) %>%
  left_join(
    stations_census %>% select(start_station, Med_Inc),
    by = "start_station"
  ) %>%
  mutate(has_census = !is.na(Med_Inc))

# Add back to trip data
indego_census <- indego %>%
  left_join(
    stations_census %>% 
      select(start_station, Med_Inc, Percent_Taking_Transit, 
             Percent_White, Total_Pop),
    by = "start_station"
  )
```

```{r join_census_to_stations2}
#| echo: false
# Prepare data for visualization
stations_for_map <- indego %>%
  distinct(start_station, start_lat, start_lon) %>%
  filter(!is.na(start_lat), !is.na(start_lon)) %>%
  left_join(
    stations_census %>% select(start_station, Med_Inc),
    by = "start_station"
  ) %>%
  mutate(has_census = !is.na(Med_Inc))

# Create the map showing problem stations
ggplot() +
  geom_sf(data = philly_census, aes(fill = Med_Inc), color = "white", size = 0.1) +
  scale_fill_viridis(
    option = "viridis",
    name = "Median\nIncome",
    labels = scales::dollar,
    na.value = "grey90"
  ) +
  # Stations with census data (small grey dots)
  geom_point(
    data = stations_for_map %>% filter(has_census),
    aes(x = start_lon, y = start_lat),
    color = "grey30", size = 1, alpha = 0.6
  ) +
  # Stations WITHOUT census data (red X marks the spot)
  geom_point(
    data = stations_for_map %>% filter(!has_census),
    aes(x = start_lon, y = start_lat),
    color = "red", size = 1, shape = 4, stroke = 1.5
  ) +
  labs(
    title = "Philadelphia Median Household Income by Census Tract",
    subtitle = "Indego stations shown (RED = no census data match)",
    caption = "Red X marks indicate stations that didn't join to census tracts"
  ) +
  mapTheme
```

# Dealing with missing data

We need to decide what to do with the non-residential bike share stations. For this example, we are going to remove them -- this is not necessarily the right way to do things always, but for the sake of simplicity, we are narrowing our scope to only stations in residential neighborhoods. We might opt to create a separate model for non-residential stations..

```{r}
# Identify which stations to keep
valid_stations <- stations_census %>%
  filter(!is.na(Med_Inc)) %>%
  pull(start_station)

# Filter trip data to valid stations only
indego_census <- indego %>%
  filter(start_station %in% valid_stations) %>%
  left_join(
    stations_census %>% 
      select(start_station, Med_Inc, Percent_Taking_Transit, 
             Percent_White, Total_Pop),
    by = "start_station"
  )

```

# Get Weather Data

Weather significantly affects bike share demand! Let's get hourly weather for Philadelphia.

```{r get_weather}
# Get weather from Philadelphia International Airport (KPHL)
# This covers Q2 2024: April 1 - June 30
weather_data <- riem_measures(
  station = "PHL",  # Philadelphia International Airport
  date_start = "2024-04-01",
  date_end = "2024-06-30"
)

# Process weather data
weather_processed <- weather_data %>%
  mutate(
    interval60 = floor_date(valid, unit = "hour"),
    Temperature = tmpf,  # Temperature in Fahrenheit
    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches
    Wind_Speed = sknt  # Wind speed in knots
  ) %>%
  select(interval60, Temperature, Precipitation, Wind_Speed) %>%
  distinct()

# Check for missing hours and interpolate if needed
weather_complete <- weather_processed %>%
  complete(interval60 = seq(min(interval60), max(interval60), by = "hour")) %>%
  fill(Temperature, Precipitation, Wind_Speed, .direction = "down")
```

## Visualize Weather Patterns

```{r visualize_weather}
#| echo: false
ggplot(weather_complete, aes(x = interval60, y = Temperature)) +
  geom_line(color = "#3182bd", alpha = 0.7) +
  geom_smooth(se = FALSE, color = "red") +
  labs(
    title = "Philadelphia Temperature - Q2 2024",
    subtitle = "Winter to early spring transition",
    x = "Date",
    y = "Temperature (°F)"
  ) +
  plotTheme
```

------------------------------------------------------------------------

# Create Space-Time Panel

## Aggregate Trips to Station-Hour Level

```{r aggregate_trips_1}
# Count trips by station-hour
trips_panel <- indego_census %>%
  group_by(interval60, start_station, start_lat, start_lon,
           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%
  summarize(Trip_Count = n()) %>%
  ungroup()
```

```{r aggregate_trips}
#| echo: false
summary_counts <- tibble(
  Metric = c(
    "Total Station-Hour Observations",
    "Unique Stations",
    "Unique Hours"
  ),
  Value = c(
    nrow(trips_panel),
    length(unique(trips_panel$start_station)),
    length(unique(trips_panel$interval60))
  )
)

summary_counts %>%
  kable(
    caption = "Summary of Panel Structure",
    col.names = c("Metric", "Value"),
    format.args = list(big.mark = ",")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

## Create Complete Panel Structure

Not every station has trips every hour. We need a **complete panel** where every station-hour combination exists (even if Trip_Count = 0).

```{r complete_panel}
# Calculate expected panel size
n_stations <- length(unique(trips_panel$start_station))
n_hours <- length(unique(trips_panel$interval60))
expected_rows <- n_stations * n_hours
```

```{r complete_panel_summary}
#| echo: false
# Create summary table
panel_summary <- tibble(
  Metric = c("Expected panel rows",
             "Current rows",
             "Missing rows"),
  Value = c(expected_rows,
            nrow(trips_panel),
            expected_rows - nrow(trips_panel))
)

# Format numbers with commas
panel_summary$Value <- format(panel_summary$Value, big.mark = ",")

# Display clean table
kable(panel_summary,
      caption = "Completeness of Panel Structure") |>
  kable_styling(bootstrap_options = c("striped", "hover"))
```

```{r complete_panel_summary_2}
# Create complete panel
study_panel <- expand.grid(
  interval60 = unique(trips_panel$interval60),
  start_station = unique(trips_panel$start_station)
) %>%
  # Join trip counts
  left_join(trips_panel, by = c("interval60", "start_station")) %>%
  # Replace NA trip counts with 0
  mutate(Trip_Count = replace_na(Trip_Count, 0))

# Fill in station attributes (they're the same for all hours)
station_attributes <- trips_panel %>%
  group_by(start_station) %>%
  summarize(
    start_lat = first(start_lat),
    start_lon = first(start_lon),
    Med_Inc = first(Med_Inc),
    Percent_Taking_Transit = first(Percent_Taking_Transit),
    Percent_White = first(Percent_White),
    Total_Pop = first(Total_Pop)
  )

study_panel <- study_panel %>%
  left_join(station_attributes, by = "start_station")
```

```{r complete_panel_summary_3}
#| echo: false
complete_panel_summary <- tibble(
Metric = "Complete panel rows",
Value = format(nrow(study_panel), big.mark = ",")
)

kable(complete_panel_summary,
caption = "Final Panel Size After Completion") |>
kable_styling(bootstrap_options = c("striped", "hover"))
```

## Add Time Features

```{r add_time_features}
study_panel <- study_panel %>%
  mutate(
    week = week(interval60),
    month = month(interval60, label = TRUE),
    dotw = wday(interval60, label = TRUE),
    hour = hour(interval60),
    date = as.Date(interval60),
    weekend = ifelse(dotw %in% c("Sat", "Sun"), 1, 0),
    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)
  )
```

## Join Weather Data

```{r join_weather}
study_panel <- study_panel %>%
  left_join(weather_complete, by = "interval60")
```

------------------------------------------------------------------------

# Create Temporal Lag Variables

The key innovation for space-time prediction: **past demand predicts future demand**.

## Why Lags?

If there were 15 bike trips from Station A at 8:00 AM, there will probably be \~15 trips at 9:00 AM. We can use this temporal persistence to improve predictions.

```{r create_lags}
# Sort by station and time
study_panel <- study_panel %>%
  arrange(start_station, interval60)

# Create lag variables WITHIN each station
study_panel <- study_panel %>%
  group_by(start_station) %>%
  mutate(
    lag1Hour = lag(Trip_Count, 1),
    lag2Hours = lag(Trip_Count, 2),
    lag3Hours = lag(Trip_Count, 3),
    lag12Hours = lag(Trip_Count, 12),
    lag1day = lag(Trip_Count, 24)
  ) %>%
  ungroup()

# Remove rows with NA lags (first 24 hours for each station)
study_panel_complete <- study_panel %>%
  filter(!is.na(lag1day))

study_panel_complete <- study_panel_complete %>%
  mutate(
    month = factor(month, levels = c("Apr", "May", "Jun")),
    dotw  = factor(dotw,  levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"))
  )
```

## Visualize Lag Correlations

```{r lag_correlations}
# Sample one station to visualize
example_station <- study_panel_complete %>%
  filter(start_station == first(start_station)) %>%
  head(168)  # One week

# Plot actual vs lagged demand
ggplot(example_station, aes(x = interval60)) +
  geom_line(aes(y = Trip_Count, color = "Current"), linewidth = 1) +
  geom_line(aes(y = lag1Hour, color = "1 Hour Ago"), linewidth = 1, alpha = 0.7) +
  geom_line(aes(y = lag1day, color = "24 Hours Ago"), linewidth = 1, alpha = 0.7) +
  scale_color_manual(values = c(
    "Current" = "#08519c",
    "1 Hour Ago" = "#3182bd",
    "24 Hours Ago" = "#6baed6"
  )) +
  labs(
    title = "Temporal Lag Patterns at One Station",
    subtitle = "Past demand predicts future demand",
    x = "Date-Time",
    y = "Trip Count",
    color = "Time Period"
  ) +
  plotTheme
```

------------------------------------------------------------------------

# Temporal Train/Test Split

**CRITICAL:** We must train on PAST data and test on FUTURE data!

## Why Temporal Validation Matters

In real operations, at 6:00 AM on March 15, we need to predict demand for March 15-31. We have data from Jan 1 - March 14, but NOT from March 15-31 (it hasn't happened yet!).

**Wrong approach:** Train on weeks 10-13, test on weeks 1-9 (predicting past from future!)

**Correct approach:** Train on weeks 1-9, test on weeks 10-13 (predicting future from past)

```{r temporal_split}
# Split by week for Q2 2024
# Q2 has weeks 14–26 (April–June)
# Train on weeks 14–21 (April–late May)
# Test on weeks 22–26 (mostly June)

train_weeks <- 14:21
test_weeks  <- 22:26

# Stations active in BOTH early (train) and late (test) periods
early_stations <- study_panel_complete %>%
  filter(week %in% train_weeks, Trip_Count > 0) %>%
  distinct(start_station) %>%
  pull(start_station)

late_stations <- study_panel_complete %>%
  filter(week %in% test_weeks, Trip_Count > 0) %>%
  distinct(start_station) %>%
  pull(start_station)

common_stations <- intersect(early_stations, late_stations)

# Keep only common stations
study_panel_q2 <- study_panel_complete %>%
  filter(start_station %in% common_stations)

# Now create train/test split
train <- study_panel_q2 %>%
  filter(week %in% train_weeks)

test <- study_panel_q2 %>%
  filter(week %in% test_weeks)
```

```{r summary 2}
#| echo: false
summary_table <- tibble(
  Set = c("Training", "Testing"),
  Observations = c(nrow(train), nrow(test)),
  Date_Range = c(
    paste(min(train$date), "to", max(train$date)),
    paste(min(test$date), "to", max(test$date))
  )
)

kable(summary_table, format = "latex", booktabs = TRUE, caption = "Train/Test Summary") %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

------------------------------------------------------------------------

# Build Predictive Models

We'll build 5 models with increasing complexity to see what improves predictions.

## Model 1: Baseline (Time + Weather)

```{r model1}

# Create day of week factor with treatment (dummy) coding
train <- train %>%
  mutate(dotw_simple = factor(dotw, levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")))

# Set contrasts to treatment coding (dummy variables)
contrasts(train$dotw_simple) <- contr.treatment(7)

# Now run the model
model1 <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,
  data = train
)

```

The model uses Monday as the baseline. Each coefficient represents the difference in expected trips per station-hour compared to Monday - dow_simple2 = Tuesday..

**Weekday Pattern (Tue-Fri):**

-   Weekday effects range from –0.063 to +0.016.
-   No weekday has a positive, strong increase, unlike your example.
-   Thursday is the only weekday above baseline.
-   Tuesday, Wednesday, and Friday are all slightly below Monday.

**Weekend Pattern (Sat-Sun):**

-   Both weekend days show substantial drops relative to Monday.
-   Sunday has the largest negative effect (–0.112).
-   This indicates significantly lower station-hour activity on weekends.

**Hourly Interpretation**

Across the 24-hour day, activity is lowest between hours 1–4, where trips fall slightly below the midnight baseline (with coefficients between -0.078 and -0.096). Activity begins to rise at hour 5 (+0.033) and increases sharply through the morning commute, with strong positive effects at hours 6–9, including a substantial morning peak at hour 8 (+0.851) and sustained high demand at hour 9 (+0.637). Midday hours (10–14) maintain moderately elevated trip counts, ranging from +0.535 to +0.643. In the afternoon, demand climbs again, rising through hours 15–16 and reaching the highest level of the entire day at hour 17 (+1.161), indicating a very strong evening peak. Activity remains high at hour 18 (+0.917) before gradually declining across the evening hours 19–22 (from +0.672 down to +0.186). By hour 23, demand tapers to a modest +0.065—still slightly above the midnight baseline but representing the transition into late-night low activity.

Isn't this fun!

## Model 2: Add Temporal Lags

```{r model2}
model2 <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +
    lag1Hour + lag3Hours + lag1day,
  data = train
)
```

The inclusion of temporal lags substantially improved the model’s explanatory power, raising the adjusted R² from approximately 0.116 to 0.348, which reflects a nearly threefold increase in explained variation. This improvement occurs because bike-share demand is highly autocorrelated: stations that experience high or low usage in one hour tend to show similar patterns in the following hours, and usage at the same hour on the previous day is also strongly predictive. The lagged variables, particularly the one-hour lag and the three-hour and one-day lags, capture these short-term and daily cyclical patterns that the hour-of-day and day-of-week predictors alone can't do. As a result, the model better reflects the underlying temporal structure of bike demand, leading to smaller residuals and a much stronger overall fit.

## Model 3: Add Demographics

```{r model3}
#| results: hide
#| message: false
#| warning: false

model3 <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +
    lag1Hour + lag3Hours + lag1day +
    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,
  data = train
)

summary(model3)
```

The addition of demographic variables did not improve model fit; in fact, the adjusted R² declined from about 0.348 in the temporal-lag model to roughly 0.237 after including median income, transit use, and racial composition. This reduction occurs primarily because adding demographics caused the loss of over 270,000 observations due to missing data, leaving the model trained on less than one-third of the original sample. With far fewer observations, the hour-to-hour temporal structure that drives trip volume becomes harder to estimate precisely, and the remaining sample may not fully represent the full behavioral patterns across the system. Although two demographic predictors, percent taking transit and percent white are statistically significant, their effect sizes are small relative to the strong temporal autocorrelation captured by the lagged variables. As a result, the demographic-augmented model explains less variance overall, not because demographics are irrelevant, but because the loss of data outweighs any modest predictive gains they could have contributed.

## Model 4: Add Station Fixed Effects

```{r model4}
model4 <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +
    lag1Hour + lag3Hours + lag1day +
    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +
    as.factor(start_station),
  data = train
)

# Summary too long with all station dummies, just show key metrics
cat("Model 4 R-squared:", summary(model4)$r.squared, "\n")
cat("Model 4 Adj R-squared:", summary(model4)$adj.r.squared, "\n")
```

Adding station fixed effects allows the model to account for persistent, station-specific differences in baseline trip demand that are not explained by hourly patterns, day-of-week effects, weather, lags, or demographics. Some stations are inherently busier because they are located in dense commercial districts, near major transit hubs, or along popular commuting corridors, while others serve quieter residential or peripheral areas with consistently lower usage. By including a dummy variable for each station, the model absorbs these stable location-level characteristics, effectively letting every station have its own intercept and preventing unobserved spatial differences from biasing the other coefficients. Although this raises the R-squared relative to the demographics-only model, the improvement is moderate because fixed effects mainly redistribute variance across stations rather than improving the model’s ability to explain within station temporal fluctuations, which are still driven primarily by the lag terms and hourly patterns.

## Model 5: Add Rush Hour Interaction

```{r model5}
train$month <- factor(train$month, levels = c("Apr", "May", "Jun"))
test$month <- factor(test$month, levels = levels(train$month))

model5 <- lm(
  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +
    lag1Hour + lag3Hours + lag1day + rush_hour + month +
    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +
    as.factor(start_station) +
    rush_hour * weekend,  # Rush hour effects different on weekends
  data = train
)
```

Model 5 adds a rush_hour × weekend interaction to allow peak-period effects to differ between weekdays and weekends. This recognizes that weekday rush hours reflect strong commuting patterns, while weekend usage is more diffuse and less time-sensitive. By including this interaction, along with month fixed effects, the model captures important behavioral differences in how riders use the system across days and times, refining the temporal structure established in earlier models.

------------------------------------------------------------------------

# Model Evaluation

## Calculate Predictions and MAE

```{r calculate_mae}
# Get predictions on test set

# Make sure month in TEST has the same levels as train
test$month <- factor(test$month, levels = model5$xlevels$month)

# Create day of week factor with treatment (dummy) coding
test <- test %>%
  mutate(dotw_simple = factor(dotw, levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")))

# Set contrasts to treatment coding (dummy variables)
contrasts(test$dotw_simple) <- contr.treatment(7)

test <- test %>%
  mutate(
    pred1 = predict(model1, newdata = test),
    pred2 = predict(model2, newdata = test),
    pred3 = predict(model3, newdata = test),
    pred4 = predict(model4, newdata = test),
    pred5 = predict(model5, newdata = test)
  )

# Calculate MAE for each model
mae_results <- data.frame(
  Model = c(
    "1. Time + Weather",
    "2. + Temporal Lags",
    "3. + Demographics",
    "4. + Station FE",
    "5. + Rush Hour Interaction"
  ),
  MAE = c(
    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),
    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),
    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),
    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),
    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)
  )
)

kable(mae_results, 
      digits = 2,
      caption = "Mean Absolute Error by Model (Test Set)",
      col.names = c("Model", "MAE (trips)")) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

## Visualize Model Comparison

```{r compare_models}
ggplot(mae_results, aes(x = reorder(Model, -MAE), y = MAE)) +
  geom_col(fill = "#3182bd", alpha = 0.8) +
  geom_text(aes(label = round(MAE, 2)), vjust = -0.5) +
  labs(
    title = "Model Performance Comparison",
    subtitle = "Lower MAE = Better Predictions",
    x = "Model",
    y = "Mean Absolute Error (trips)"
  ) +
  plotTheme +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

**Question:** Which features gave us the biggest improvement?

For Q2 2024, the features that helped the most were the temporal lags (Model 2). When we added the 1-hour, 3-hour, and 24-hour lag variables, the MAE dropped the most, from 0.86 down to 0.71. This makes sense because bike demand tends to repeat patterns from the previous hour or the same hour yesterday.

------------------------------------------------------------------------

# Space-Time Error Analysis

## Observed vs. Predicted

Let's use our best model (Model 2) for error analysis.

```{r obs_vs_pred}
test <- test %>%
  mutate(
    error = Trip_Count - pred2,
    abs_error = abs(error),
    time_of_day = case_when(
      hour < 7 ~ "Overnight",
      hour >= 7 & hour < 10 ~ "AM Rush",
      hour >= 10 & hour < 15 ~ "Mid-Day",
      hour >= 15 & hour <= 18 ~ "PM Rush",
      hour > 18 ~ "Evening"
    )
  )

# Scatter plot by time and day type
ggplot(test, aes(x = Trip_Count, y = pred2)) +
  geom_point(alpha = 0.2, color = "#3182bd") +
  geom_abline(slope = 1, intercept = 0, color = "red", linewidth = 1) +
  geom_smooth(method = "lm", se = FALSE, color = "darkgreen") +
  facet_grid(weekend ~ time_of_day) +
  labs(
    title = "Observed vs. Predicted Bike Trips",
    subtitle = "Model 2 performance by time period",
    x = "Observed Trips",
    y = "Predicted Trips",
    caption = "Red line = perfect predictions; Green line = actual model fit"
  ) +
  plotTheme
```

**Question:** Where is the model performing well? Where is it struggling?

The model performs best during Mid-Day and Overnight hours, where points fall close to the 45° line and the green fitted line tracks the red perfect-prediction line more closely. It performs worst during AM Rush and PM Rush, especially on weekdays, where the model systematically under-predicts higher ridership and shows much wider scatter. Evening periods also show moderate under-prediction but less extreme than the rush hours.

## Spatial Error Patterns

Are prediction errors clustered in certain parts of Philadelphia?

```{r spatial_errors}
# Calculate MAE by station
station_errors <- test %>%
  group_by(start_station, start_lat.x, start_lon.y) %>%
  summarize(
    MAE = mean(abs_error, na.rm = TRUE),
    avg_demand = mean(Trip_Count, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  filter(!is.na(start_lat.x), !is.na(start_lon.y))

## Create Two Maps Side-by-Side with Proper Legends (sorry these maps are ugly)

# Calculate station errors
station_errors <- test %>%
  filter(!is.na(pred2)) %>%
  group_by(start_station, start_lat.x, start_lon.y) %>%
  summarize(
    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),
    avg_demand = mean(Trip_Count, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  filter(!is.na(start_lat.x), !is.na(start_lon.y))

# Map 1: Prediction Errors
p1 <- ggplot() +
  geom_sf(data = philly_census, fill = "grey95", color = "white", size = 0.2) +
  geom_point(
    data = station_errors,
    aes(x = start_lon, y = start_lat, color = MAE),
    size = 3.5,
    alpha = 0.7
  ) +
  scale_color_viridis(
    option = "plasma",
    name = "MAE\n(trips)",
    direction = -1,
    breaks = c(0.5, 1.0, 1.5),  # Fewer, cleaner breaks
    labels = c("0.5", "1.0", "1.5")
  ) +
  labs(title = "Prediction Errors",
       subtitle = "Higher in Center City") +
  mapTheme +
  theme(
    legend.position = "right",
    legend.title = element_text(size = 10, face = "bold"),
    legend.text = element_text(size = 9),
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 10)
  ) +
  guides(color = guide_colorbar(
    barwidth = 1.5,
    barheight = 12,
    title.position = "top",
    title.hjust = 0.5
  ))

# Map 2: Average Demand
p2 <- ggplot() +
  geom_sf(data = philly_census, fill = "grey95", color = "white", size = 0.2) +
  geom_point(
    data = station_errors,
    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),
    size = 3.5,
    alpha = 0.7
  ) +
  scale_color_viridis(
    option = "viridis",
    name = "Avg\nDemand",
    direction = -1,
    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),  # Clear breaks
    labels = c("0.5", "1.0", "1.5", "2.0", "2.5")
  ) +
  labs(title = "Average Demand",
       subtitle = "Trips per station-hour") +
  mapTheme +
  theme(
    legend.position = "right",
    legend.title = element_text(size = 10, face = "bold"),
    legend.text = element_text(size = 9),
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 10)
  ) +
  guides(color = guide_colorbar(
    barwidth = 1.5,
    barheight = 12,
    title.position = "top",
    title.hjust = 0.5
  ))

# Map 1: Prediction Errors
p1 <- ggplot() +
  geom_sf(data = philly_census, fill = "grey95", color = "white", size = 0.1) +
  geom_point(
    data = station_errors,
    aes(x = start_lon.y, y = start_lat.x, color = MAE),
    size = 3.5,
    alpha = 0.7
  ) +
  scale_color_viridis(
    option = "plasma",
    name = "MAE (trips)",
    direction = -1,
    breaks = c(0.5, 1.0, 1.5),
    labels = c("0.5", "1.0", "1.5")
  ) +
  labs(title = "Prediction Errors") +
  mapTheme +
  theme(
    legend.position = "bottom",
    legend.title = element_text(size = 10, face = "bold"),
    legend.text = element_text(size = 9),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5)
  ) +
  guides(color = guide_colorbar(
    barwidth = 12,
    barheight = 1,
    title.position = "top",
    title.hjust = 0.5
  ))

# Map 2: Average Demand  
p2 <- ggplot() +
  geom_sf(data = philly_census, fill = "grey95", color = "white", size = 0.1) +
  geom_point(
    data = station_errors,
    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),
    size = 3.5,
    alpha = 0.7
  ) +
  scale_color_viridis(
    option = "viridis",
    name = "Avg Demand (trips/hour)",
    direction = -1,
    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),
    labels = c("0.5", "1.0", "1.5", "2.0", "2.5")
  ) +
  labs(title = "Average Demand") +
  mapTheme +
  theme(
    legend.position = "bottom",
    legend.title = element_text(size = 10, face = "bold"),
    legend.text = element_text(size = 9),
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5)
  ) +
  guides(color = guide_colorbar(
    barwidth = 12,
    barheight = 1,
    title.position = "top",
    title.hjust = 0.5
  ))

# Combine
grid.arrange(
  p1, p2,
  ncol = 2
  )
```

**Question:** Do you see spatial clustering of errors? What neighborhoods have high errors?

Yes, there is a clear spatial clustering of errors. The highest prediction errors occur in and around Center City and University City, where demand is also highest. Outlying neighborhoods such as South Philly, West Parkside, and Northeast Philly show lower errors because demand is lower and more stable.

## Temporal Error Patterns

When are we most wrong?

```{r temporal_errors}
# MAE by time of day and day type
temporal_errors <- test %>%
  group_by(time_of_day, weekend) %>%
  summarize(
    MAE = mean(abs_error, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(day_type = ifelse(weekend == 1, "Weekend", "Weekday"))

ggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +
  geom_col(position = "dodge") +
  scale_fill_manual(values = c("Weekday" = "#08519c", "Weekend" = "#6baed6")) +
  labs(
    title = "Prediction Errors by Time Period",
    subtitle = "When is the model struggling most?",
    x = "Time of Day",
    y = "Mean Absolute Error (trips)",
    fill = "Day Type"
  ) +
  plotTheme +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The model performs best during the overnight and evening hours, when demand is low and stable. Errors increase during the middle of the day and are highest during the AM and PM rush periods, especially the PM commute. This makes sense because rush-hour ridership is far more volatile and sensitive to small changes in weather, work schedules, and travel patterns.

## Errors and Demographics

Are prediction errors related to neighborhood characteristics?

```{r errors_demographics}
# Join demographic data to station errors
station_errors_demo <- station_errors %>%
  left_join(
    station_attributes %>% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),
    by = "start_station"
  ) %>%
  filter(!is.na(Med_Inc))

# Create plots
p1 <- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +
  geom_point(alpha = 0.5, color = "#3182bd") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  scale_x_continuous(labels = scales::dollar) +
  labs(title = "Errors vs. Median Income", x = "Median Income", y = "MAE") +
  plotTheme

p2 <- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +
  geom_point(alpha = 0.5, color = "#3182bd") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Errors vs. Transit Usage", x = "% Taking Transit", y = "MAE") +
  plotTheme

p3 <- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +
  geom_point(alpha = 0.5, color = "#3182bd") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(title = "Errors vs. Race", x = "% White", y = "MAE") +
  plotTheme

grid.arrange(p1, p2, p3, ncol = 2)
```

**Critical Question:** Are prediction errors systematically higher in certain demographic groups? What are the equity implications?

Across Philadelphia, prediction errors show meaningful relationships with neighborhood demographics. Errors tend to be slightly higher in stations located in higher-income or higher-percentage-White areas, suggesting the model struggles more in wealthier, less transit-dependent neighborhoods where trip patterns may be more irregular. In contrast, stations in areas with higher transit use show lower errors, likely because these neighborhoods have more stable, predictable travel demand. These patterns raise equity considerations: if models systematically underperform in certain demographic groups, operational decisions (like rebalancing) could unintentionally favor or disadvantage specific communities.

------------------------------------------------------------------------

# Part 2: COMPARE RESULTS BETWEEN Q1 2025 AND Q2 2024

## Compare MAE Across Quarters

```{r compare_mae_q1_q2}
mae_results_q2 <- mae_results %>%
  mutate(Quarter = "Q2 2024")

mae_results_q1_labeled <- mae_results_q1 %>%
  mutate(Quarter = "Q1 2025")

mae_compare <- bind_rows(mae_results_q2, mae_results_q1_labeled)

kable(mae_compare,
      digits = 2,
      caption = "Model MAE Comparison: Q1 2025 vs Q2 2024",
      col.names = c("Model", "MAE (trips)", "Quarter")) %>%
  kable_styling(bootstrap_options = c("striped","hover"))
```

## MAE Comparison Plot

```{r compare_mae_plot_q1_q2}
ggplot(mae_compare,
       aes(x = Model, y = MAE, fill = Quarter)) +
  geom_col(position = "dodge") +
  geom_text(aes(label = round(MAE, 2)),
            position = position_dodge(width = 0.9),
            vjust = -0.4, size = 3) +
  labs(
    title = "Model Performance Across Quarters",
    y     = "Mean Absolute Error (trips)",
    x     = "Model"
  ) +
  plotTheme +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Models consistently perform better in Q1 2025 than in Q2 2024, with lower MAE across every specification. Winter travel is more predictable because most trips are work-related and follow a routine. In Q2 2024, ridership is higher and more random due to nicer weather, outdoor activities, and tourism, which makes predictions harder. This is why every model has lower MAE in Q1 than Q2.

## Temporal Error Comparison: Q1 vs Q2

```{r compare_temporal_errors_q1_q2}

#Q1 2025 TEMPORAL ERRORS
temporal_errors_q1 <- test_q1 %>%
  group_by(time_of_day_q1, weekend) %>%
  summarize(
    MAE = mean(abs_error_q1, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    Day_Type = ifelse(weekend == 1, "Weekend", "Weekday"),
    Quarter = "Q1 2025",
    Time_of_Day = time_of_day_q1
  ) %>%
  select(Time_of_Day, MAE, Day_Type, Quarter)


#Q2 2024 TEMPORAL ERRORS
temporal_errors_q2 <- test %>%        # your Q2 dataset
  group_by(time_of_day, weekend) %>%
  summarize(
    MAE = mean(abs_error, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(
    Day_Type = ifelse(weekend == 1, "Weekend", "Weekday"),
    Quarter = "Q2 2024",
    Time_of_Day = time_of_day
  ) %>%
  select(Time_of_Day, MAE, Day_Type, Quarter)


#COMBINE
temporal_compare <- bind_rows(temporal_errors_q1, temporal_errors_q2)


#PLOT
ggplot(temporal_compare,
       aes(x = Time_of_Day, y = MAE, fill = Day_Type)) +
  geom_col(position = "dodge") +
  facet_wrap(~ Quarter, ncol = 2) +
  labs(
    title = "Prediction Errors by Time of Day and Quarter",
    y = "Mean Absolute Error (trips)",
    x = "Time of Day",
    fill = "Day Type"
  ) +
  plotTheme +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

Q1 and Q2 have different error patterns. In Q1 2025, errors spike during morning and evening rush hours, when travel is busiest and most unpredictable. Overnight and evening hours are easiest to predict. In Q2 2024, errors are more evenly spread throughout the day because people ride more for leisure, not just commuting. Seasonal differences, like winter vs. spring, explain most of the pattern.

## Feature Importance Comparison

```{r compare_feature_importance_q1_q2}
get_coef_table <- function(model, quarter_label) {
  sm <- summary(model)$coefficients
  tibble(
    Variable  = rownames(sm),
    Estimate  = sm[, 1]
  ) %>%
    mutate(
      Quarter  = quarter_label,
      Abs_Est  = abs(Estimate)
    )
}

coefs_q2 <- get_coef_table(model2,   "Q2 2024")
coefs_q1 <- get_coef_table(model2_q1,"Q1 2025")

vars_focus <- c("Temperature", "Precipitation",
                "lag1Hour", "lag3Hours", "lag1day")

feature_compare <- bind_rows(coefs_q2, coefs_q1) %>%
  filter(Variable %in% vars_focus) %>%
  arrange(Quarter, desc(Abs_Est))

kable(feature_compare,
      digits = 3,
      caption = "Key Coefficient Magnitudes in Model 2 by Quarter") %>%
  kable_styling(bootstrap_options = c("striped","hover"))
```

Different features matter in each quarter. In Q1 2025, precipitation is the strongest predictor because winter weather sharply affects ridership. Temporal lags also matter because demand is more consistent hour to hour. In Q2 2024, lag effects are still important, but weather has a smaller impact, and trips follow more flexible patterns. Overall, winter demand depends more on weather, while spring demand depends more on regular daily patterns.

------------------------------------------------------------------------

# Part 3: FEATURE ENGINEERING AND MODEL IMPROVEMENT

```{r feature_engineering}
study_panel_complete_fe <- study_panel_complete %>%
arrange(start_station, interval60) %>%
group_by(start_station) %>%
mutate(
center_city = if_else(
start_lon.x >= -75.18 & start_lon.x <= -75.13 &
start_lat.x >= 39.94 & start_lat.x <= 39.96,
1, 0, missing = 0
),
rush_demand = rush_hour * Trip_Count,
lag1week = lag(Trip_Count, 24 * 7),
roll7day = rollmean(Trip_Count, k = 24 * 7,
align = "right", fill = NA)
) %>%
ungroup() %>%
mutate(
rush_demand = replace_na(rush_demand, 0),
lag1week = replace_na(lag1week, 0),
roll7day = replace_na(roll7day, 0)
)

#Match train/test splits using the same rows as original objects

train_fe <- study_panel_complete_fe %>%
semi_join(train %>% select(interval60, start_station),
by = c("interval60", "start_station"))

test_fe <- study_panel_complete_fe %>%
semi_join(test %>% select(interval60, start_station),
by = c("interval60", "start_station"))
```

# Build Improved Model (Model 2 with New Features)

```{r improved_model}
model2_fe <- lm(
Trip_Count ~
as.factor(hour) + dotw +
Temperature + Precipitation +
lag1Hour + lag3Hours + lag1day +
center_city + rush_demand +
lag1week + roll7day,
data = train_fe
)
```

# Predict on test

```{r predict_on_test}
test_fe <- test_fe %>%
mutate(pred_fe = predict(model2_fe, newdata = .))
```

#MAE Comparison Old vs New model 2

```{r mae_oldvsnew}
mae_old <- mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE)
mae_new <- mean(abs(test_fe$Trip_Count - test_fe$pred_fe), na.rm = TRUE)

mae_compare_fe <- tibble(
Model = c("Original Model 2", "Improved Model 2 + New Features"),
MAE = c(mae_old, mae_new)
)

kable(mae_compare_fe,
caption = "MAE Comparison: Original vs Improved Model") %>%
kable_styling(bootstrap_options = c("striped","hover"))
```

#Plot

```{r plot_new}
ggplot(mae_compare_fe, aes(x = Model, y = MAE, fill = Model)) +
geom_col() +
geom_text(aes(label = round(MAE, 3)), vjust = -0.5) +
labs(
title = "Model Improvement After Feature Engineering",
y = "MAE (trips)"
) +
plotTheme +
theme(legend.position = "none")
```

Model 2 already included the main predictors like hour of day, day of week, weather, and a few short-term lags. But after looking at the error analysis, it was pretty clear that the model was still missing some important patterns. So I added four new features that directly targeted the biggest problems I found.

The first feature I added was a Center City indicator, because the error maps showed the model was consistently under-predicting in the downtown stations. These stations have way more activity than the rest of the city, so giving the model a simple 0/1 flag helps it separate the normal stations from the very busy ones.

Next, I added rush_demand, which is rush hour × recent demand. The temporal plots showed that most of the big mistakes happened during the morning and evening commute times. This feature tells the model when a station’s demand tends to spike specifically during rush hour, which helps reduce under-prediction during those busy periods.

I also added lag1week, which is the same hour from the previous week. The data showed strong weekly patterns. The original model only looked back a few hours or one day, so adding this weekly lag helps the model pick up those repeating patterns.

And last, I added roll7day, the 7-day rolling average. Hour-to-hour demand can be really noisy, so this gives the model a smoother signal showing whether demand at a station has been trending up or down over the last week. This helps make predictions more stable instead of jumping around from one random spike.

Together, these four features directly address the main issues found in the error analysis and help the model make more consistent and accurate predictions.

------------------------------------------------------------------------

## Part 4: Critical Reflection

1.  **Operational implications:** Even though my improved model has a fairly low MAE, it still isn’t perfect, and Indego would need to be careful about relying on it. Most of the time, an MAE under 1 trip is “good enough” to guide rebalancing decisions, but errors during rush hour or at busy downtown stations can still cause real problems. A station emptying out 2–3 bikes earlier than expected can affect commuters pretty quickly. So I would recommend using this model as a supportive tool rather than something that fully automates rebalancing. It works best for planning ahead and spotting likely hotspots, but staff should still monitor real-time conditions, weather, and special events.

2.  **Equity considerations:** My error analysis showed that prediction accuracy wasn’t equal across all neighborhoods. Some areas, like Center City and higher-income stations had larger error, which means the model could unintentionally over or under serve certain parts of the city. If Indego relied on these predictions without oversight, it could reinforce existing inequalities in bike access by sending more rebalancing resources to already advantaged areas.

3.  **Model limitations:** The model doesn’t account for special events, tourism spikes, construction, transit delays, or sudden weather changes, all of which can dramatically change bike demand. It also assumes past patterns repeat in the same way every week, which isn’t always true. If I could add more features, I would add real-time weather forecasts, event calendars, and bike lane accessibility.

------------------------------------------------------------------------
